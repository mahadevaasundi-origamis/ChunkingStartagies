{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef893960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chonkie[all]\n",
      "  Downloading chonkie-1.4.2-cp312-cp312-win_amd64.whl.metadata (24 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chonkie[all]) (4.67.1)\n",
      "Requirement already satisfied: numpy>=2.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chonkie[all]) (2.2.6)\n",
      "Requirement already satisfied: tokenizers>=0.16.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chonkie[all]) (0.22.1)\n",
      "Requirement already satisfied: tiktoken>=0.5.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chonkie[all]) (0.12.0)\n",
      "Requirement already satisfied: rich>=13.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chonkie[all]) (14.2.0)\n",
      "Collecting tree-sitter>=0.20.0 (from chonkie[all])\n",
      "  Downloading tree_sitter-0.25.2-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Collecting tree-sitter-language-pack>=0.7.0 (from chonkie[all])\n",
      "  Downloading tree_sitter_language_pack-0.11.0-cp310-abi3-win_amd64.whl.metadata (19 kB)\n",
      "Collecting magika<1.1.0,>=0.6.0 (from chonkie[all])\n",
      "  Using cached magika-1.0.1-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sentence-transformers>=3.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chonkie[all]) (5.1.2)\n",
      "Requirement already satisfied: openai>=1.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chonkie[all]) (2.8.1)\n",
      "Collecting model2vec>=0.3.0 (from chonkie[all])\n",
      "  Downloading model2vec-0.7.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting cohere>=5.13.0 (from chonkie[all])\n",
      "  Downloading cohere-5.20.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting voyageai>=0.3.2 (from chonkie[all])\n",
      "  Downloading voyageai-0.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: accelerate>=1.6.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chonkie[all]) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chonkie[all]) (0.36.0)\n",
      "Requirement already satisfied: jsonschema>=4.23.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chonkie[all]) (4.25.1)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chonkie[all]) (2.12.4)\n",
      "Collecting google-genai>=1.0.0 (from chonkie[all])\n",
      "  Downloading google_genai-1.52.0-py3-none-any.whl.metadata (46 kB)\n",
      "Requirement already satisfied: transformers>=4.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chonkie[all]) (4.57.1)\n",
      "Requirement already satisfied: torch<3.0,>=2.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chonkie[all]) (2.9.1)\n",
      "Requirement already satisfied: chromadb>=1.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chonkie[all]) (1.3.5)\n",
      "Collecting qdrant-client>=1.0.0 (from chonkie[all])\n",
      "  Downloading qdrant_client-1.16.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting turbopuffer>=0.2.0 (from turbopuffer[fast]>=0.2.0; extra == \"all\"->chonkie[all])\n",
      "  Downloading turbopuffer-1.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting weaviate-client>=4.16.7 (from chonkie[all])\n",
      "  Downloading weaviate_client-4.18.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting azure-identity>=1.23.0 (from chonkie[all])\n",
      "  Downloading azure_identity-1.25.1-py3-none-any.whl.metadata (88 kB)\n",
      "Collecting pinecone (from chonkie[all])\n",
      "  Downloading pinecone-8.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pymongo (from chonkie[all])\n",
      "  Downloading pymongo-4.15.4-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
      "Collecting vecs>=0.4.0 (from chonkie[all])\n",
      "  Downloading vecs-0.4.5.tar.gz (22 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: pandas in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chonkie[all]) (2.2.3)\n",
      "Requirement already satisfied: tabulate in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chonkie[all]) (0.9.0)\n",
      "Collecting openpyxl (from chonkie[all])\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting elasticsearch>=8.0.0 (from chonkie[all])\n",
      "  Downloading elasticsearch-9.2.0-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting pymilvus>=2.1.0 (from chonkie[all])\n",
      "  Downloading pymilvus-2.6.3-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: click>=8.1.7 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from magika<1.1.0,>=0.6.0->chonkie[all]) (8.3.1)\n",
      "Requirement already satisfied: onnxruntime>=1.17.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from magika<1.1.0,>=0.6.0->chonkie[all]) (1.23.2)\n",
      "Collecting onnxruntime>=1.17.0 (from magika<1.1.0,>=0.6.0->chonkie[all])\n",
      "  Using cached onnxruntime-1.20.1-cp312-cp312-win_amd64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=1.0.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from magika<1.1.0,>=0.6.0->chonkie[all]) (1.2.1)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from onnxruntime>=1.17.0->magika<1.1.0,>=0.6.0->chonkie[all]) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from onnxruntime>=1.17.0->magika<1.1.0,>=0.6.0->chonkie[all]) (25.9.23)\n",
      "Requirement already satisfied: packaging in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from onnxruntime>=1.17.0->magika<1.1.0,>=0.6.0->chonkie[all]) (25.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from onnxruntime>=1.17.0->magika<1.1.0,>=0.6.0->chonkie[all]) (6.33.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from onnxruntime>=1.17.0->magika<1.1.0,>=0.6.0->chonkie[all]) (1.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from torch<3.0,>=2.0.0->chonkie[all]) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from torch<3.0,>=2.0.0->chonkie[all]) (4.15.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from torch<3.0,>=2.0.0->chonkie[all]) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from torch<3.0,>=2.0.0->chonkie[all]) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from torch<3.0,>=2.0.0->chonkie[all]) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from torch<3.0,>=2.0.0->chonkie[all]) (80.9.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from accelerate>=1.6.0->chonkie[all]) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from accelerate>=1.6.0->chonkie[all]) (6.0.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from accelerate>=1.6.0->chonkie[all]) (0.7.0)\n",
      "Collecting azure-core>=1.31.0 (from azure-identity>=1.23.0->chonkie[all])\n",
      "  Using cached azure_core-1.36.0-py3-none-any.whl.metadata (47 kB)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from azure-identity>=1.23.0->chonkie[all]) (46.0.3)\n",
      "Collecting msal>=1.30.0 (from azure-identity>=1.23.0->chonkie[all])\n",
      "  Using cached msal-1.34.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity>=1.23.0->chonkie[all])\n",
      "  Using cached msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from azure-core>=1.31.0->azure-identity>=1.23.0->chonkie[all]) (2.32.5)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chromadb>=1.0.0->chonkie[all]) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chromadb>=1.0.0->chonkie[all]) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.0->chonkie[all]) (0.38.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chromadb>=1.0.0->chonkie[all]) (5.4.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chromadb>=1.0.0->chonkie[all]) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chromadb>=1.0.0->chonkie[all]) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chromadb>=1.0.0->chonkie[all]) (1.38.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chromadb>=1.0.0->chonkie[all]) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chromadb>=1.0.0->chonkie[all]) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chromadb>=1.0.0->chonkie[all]) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chromadb>=1.0.0->chonkie[all]) (1.76.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chromadb>=1.0.0->chonkie[all]) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chromadb>=1.0.0->chonkie[all]) (0.20.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chromadb>=1.0.0->chonkie[all]) (33.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chromadb>=1.0.0->chonkie[all]) (9.1.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chromadb>=1.0.0->chonkie[all]) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chromadb>=1.0.0->chonkie[all]) (3.11.4)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chromadb>=1.0.0->chonkie[all]) (0.28.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.0->chonkie[all]) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.0->chonkie[all]) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.0->chonkie[all]) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.0->chonkie[all]) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity>=1.23.0->chonkie[all]) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity>=1.23.0->chonkie[all]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity>=1.23.0->chonkie[all]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity>=1.23.0->chonkie[all]) (2025.11.12)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from build>=1.0.3->chromadb>=1.0.0->chonkie[all]) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from build>=1.0.3->chromadb>=1.0.0->chonkie[all]) (0.4.6)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere>=5.13.0->chonkie[all])\n",
      "  Downloading fastavro-1.12.1-cp312-cp312-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting httpx-sse==0.4.0 (from cohere>=5.13.0->chonkie[all])\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from cohere>=5.13.0->chonkie[all]) (2.41.5)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere>=5.13.0->chonkie[all])\n",
      "  Downloading types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from cryptography>=2.5->azure-identity>=1.23.0->chonkie[all]) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from cffi>=2.0.0->cryptography>=2.5->azure-identity>=1.23.0->chonkie[all]) (2.23)\n",
      "Requirement already satisfied: anyio in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from elasticsearch>=8.0.0->chonkie[all]) (4.11.0)\n",
      "Collecting elastic-transport<10,>=9.2.0 (from elasticsearch>=8.0.0->chonkie[all])\n",
      "  Downloading elastic_transport-9.2.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from elasticsearch>=8.0.0->chonkie[all]) (1.3.1)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from google-genai>=1.0.0->chonkie[all]) (2.43.0)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from google-genai>=1.0.0->chonkie[all]) (15.0.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai>=1.0.0->chonkie[all]) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai>=1.0.0->chonkie[all]) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai>=1.0.0->chonkie[all]) (4.9.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb>=1.0.0->chonkie[all]) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.0->chonkie[all]) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from pydantic>=2.0.0->chonkie[all]) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from pydantic>=2.0.0->chonkie[all]) (0.4.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-genai>=1.0.0->chonkie[all]) (0.6.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from jsonschema>=4.23.0->chonkie[all]) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from jsonschema>=4.23.0->chonkie[all]) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from jsonschema>=4.23.0->chonkie[all]) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from jsonschema>=4.23.0->chonkie[all]) (0.29.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.0->chonkie[all]) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.0->chonkie[all]) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.0->chonkie[all]) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=1.0.0->chonkie[all]) (0.10)\n",
      "Requirement already satisfied: joblib in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from model2vec>=0.3.0->chonkie[all]) (1.5.2)\n",
      "Collecting PyJWT<3,>=1.0.0 (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity>=1.23.0->chonkie[all])\n",
      "  Using cached PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from openai>=1.0.0->chonkie[all]) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.0->chonkie[all]) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.0->chonkie[all]) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.0->chonkie[all]) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.0->chonkie[all]) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.38.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.0->chonkie[all]) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.0->chonkie[all]) (0.59b0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from pandas->chonkie[all]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from pandas->chonkie[all]) (2025.2)\n",
      "Collecting portalocker<4.0,>=2.7.0 (from qdrant-client>=1.0.0->chonkie[all])\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting pywin32>=226 (from portalocker<4.0,>=2.7.0->qdrant-client>=1.0.0->chonkie[all])\n",
      "  Using cached pywin32-311-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client>=1.0.0->chonkie[all])\n",
      "  Downloading h2-4.3.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.0.0->chonkie[all])\n",
      "  Downloading hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.0.0->chonkie[all])\n",
      "  Downloading hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from rich>=13.0.0->chonkie[all]) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from rich>=13.0.0->chonkie[all]) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13.0.0->chonkie[all]) (0.1.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers>=3.0.0->chonkie[all]) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers>=3.0.0->chonkie[all]) (1.16.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers>=3.0.0->chonkie[all]) (12.0.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from transformers>=4.0.0->chonkie[all]) (2025.11.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sympy->onnxruntime>=1.17.0->magika<1.1.0,>=0.6.0->chonkie[all]) (1.3.0)\n",
      "Collecting tree-sitter-c-sharp>=0.23.1 (from tree-sitter-language-pack>=0.7.0->chonkie[all])\n",
      "  Downloading tree_sitter_c_sharp-0.23.1-cp39-abi3-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting tree-sitter-embedded-template>=0.25.0 (from tree-sitter-language-pack>=0.7.0->chonkie[all])\n",
      "  Downloading tree_sitter_embedded_template-0.25.0-cp310-abi3-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting tree-sitter-yaml>=0.7.2 (from tree-sitter-language-pack>=0.7.0->chonkie[all])\n",
      "  Downloading tree_sitter_yaml-0.7.2-cp310-abi3-win_amd64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: aiohttp<4,>=3.10.11 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from turbopuffer>=0.2.0->turbopuffer[fast]>=0.2.0; extra == \"all\"->chonkie[all]) (3.13.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4,>=3.10.11->turbopuffer>=0.2.0->turbopuffer[fast]>=0.2.0; extra == \"all\"->chonkie[all]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4,>=3.10.11->turbopuffer>=0.2.0->turbopuffer[fast]>=0.2.0; extra == \"all\"->chonkie[all]) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4,>=3.10.11->turbopuffer>=0.2.0->turbopuffer[fast]>=0.2.0; extra == \"all\"->chonkie[all]) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4,>=3.10.11->turbopuffer>=0.2.0->turbopuffer[fast]>=0.2.0; extra == \"all\"->chonkie[all]) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4,>=3.10.11->turbopuffer>=0.2.0->turbopuffer[fast]>=0.2.0; extra == \"all\"->chonkie[all]) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4,>=3.10.11->turbopuffer>=0.2.0->turbopuffer[fast]>=0.2.0; extra == \"all\"->chonkie[all]) (1.22.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from typer>=0.9.0->chromadb>=1.0.0->chonkie[all]) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.0->chonkie[all]) (0.7.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.0->chonkie[all]) (1.1.1)\n",
      "Collecting pgvector==0.3.* (from vecs>=0.4.0->chonkie[all])\n",
      "  Downloading pgvector-0.3.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: sqlalchemy==2.* in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from vecs>=0.4.0->chonkie[all]) (2.0.44)\n",
      "Collecting psycopg2-binary==2.9.* (from vecs>=0.4.0->chonkie[all])\n",
      "  Downloading psycopg2_binary-2.9.11-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting flupy==1.* (from vecs>=0.4.0->chonkie[all])\n",
      "  Downloading flupy-1.2.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: deprecated==1.2.* in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from vecs>=0.4.0->chonkie[all]) (1.2.18)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from deprecated==1.2.*->vecs>=0.4.0->chonkie[all]) (1.17.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sqlalchemy==2.*->vecs>=0.4.0->chonkie[all]) (3.2.4)\n",
      "Collecting aiolimiter (from voyageai>=0.3.2->chonkie[all])\n",
      "  Downloading aiolimiter-1.2.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters>=0.3.8 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from voyageai>=0.3.2->chonkie[all]) (1.0.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-text-splitters>=0.3.8->voyageai>=0.3.2->chonkie[all]) (1.0.7)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters>=0.3.8->voyageai>=0.3.2->chonkie[all]) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters>=0.3.8->voyageai>=0.3.2->chonkie[all]) (0.4.44)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters>=0.3.8->voyageai>=0.3.2->chonkie[all]) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters>=0.3.8->voyageai>=0.3.2->chonkie[all]) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters>=0.3.8->voyageai>=0.3.2->chonkie[all]) (0.25.0)\n",
      "Collecting validators<1.0.0,>=0.34.0 (from weaviate-client>=4.16.7->chonkie[all])\n",
      "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting authlib<2.0.0,>=1.2.1 (from weaviate-client>=4.16.7->chonkie[all])\n",
      "  Downloading authlib-1.6.5-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting deprecation<3.0.0,>=2.1.0 (from weaviate-client>=4.16.7->chonkie[all])\n",
      "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.17.0->magika<1.1.0,>=0.6.0->chonkie[all]) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.17.0->magika<1.1.0,>=0.6.0->chonkie[all]) (3.5.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from jinja2->torch<3.0,>=2.0.0->chonkie[all]) (3.0.3)\n",
      "Collecting et-xmlfile (from openpyxl->chonkie[all])\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pinecone-plugin-assistant<4.0.0,>=3.0.1 (from pinecone->chonkie[all])\n",
      "  Downloading pinecone_plugin_assistant-3.0.1-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting pinecone-plugin-interface<0.1.0,>=0.0.7 (from pinecone->chonkie[all])\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting packaging (from onnxruntime>=1.17.0->magika<1.1.0,>=0.6.0->chonkie[all])\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo->chonkie[all])\n",
      "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=3.0.0->chonkie[all]) (3.6.0)\n",
      "Downloading chonkie-1.4.2-cp312-cp312-win_amd64.whl (537 kB)\n",
      "   ---------------------------------------- 0.0/537.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 537.0/537.0 kB 2.5 MB/s  0:00:00\n",
      "Downloading magika-1.0.1-py3-none-win_amd64.whl (12.7 MB)\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.7 MB 4.2 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.0/12.7 MB 3.6 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.1/12.7 MB 3.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.1/12.7 MB 3.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.6/12.7 MB 2.6 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 3.1/12.7 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 4.2/12.7 MB 2.9 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 4.2/12.7 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 5.2/12.7 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 5.2/12.7 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 5.2/12.7 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 5.2/12.7 MB 2.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 6.3/12.7 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 7.3/12.7 MB 2.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 7.3/12.7 MB 2.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.4/12.7 MB 2.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 8.4/12.7 MB 2.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 9.4/12.7 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 9.4/12.7 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.5/12.7 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.7 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.7 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.7 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.7 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.7/12.7 MB 2.4 MB/s  0:00:05\n",
      "Using cached onnxruntime-1.20.1-cp312-cp312-win_amd64.whl (11.3 MB)\n",
      "Downloading azure_identity-1.25.1-py3-none-any.whl (191 kB)\n",
      "Using cached azure_core-1.36.0-py3-none-any.whl (213 kB)\n",
      "Downloading cohere-5.20.0-py3-none-any.whl (303 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading fastavro-1.12.1-cp312-cp312-win_amd64.whl (444 kB)\n",
      "Downloading types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\n",
      "Downloading elasticsearch-9.2.0-py3-none-any.whl (960 kB)\n",
      "   ---------------------------------------- 0.0/960.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 960.5/960.5 kB 4.4 MB/s  0:00:00\n",
      "Downloading elastic_transport-9.2.0-py3-none-any.whl (65 kB)\n",
      "Downloading google_genai-1.52.0-py3-none-any.whl (261 kB)\n",
      "Downloading model2vec-0.7.0-py3-none-any.whl (53 kB)\n",
      "Using cached msal-1.34.0-py3-none-any.whl (116 kB)\n",
      "Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Using cached msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
      "Downloading pymilvus-2.6.3-py3-none-any.whl (273 kB)\n",
      "Downloading qdrant_client-1.16.0-py3-none-any.whl (328 kB)\n",
      "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Downloading h2-4.3.0-py3-none-any.whl (61 kB)\n",
      "Downloading hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Downloading hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Using cached pywin32-311-cp312-cp312-win_amd64.whl (9.5 MB)\n",
      "Downloading tree_sitter-0.25.2-cp312-cp312-win_amd64.whl (127 kB)\n",
      "Downloading tree_sitter_language_pack-0.11.0-cp310-abi3-win_amd64.whl (16.2 MB)\n",
      "   ---------------------------------------- 0.0/16.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.0/16.2 MB 16.7 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 2.1/16.2 MB 4.7 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 3.1/16.2 MB 4.9 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 3.1/16.2 MB 4.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.9/16.2 MB 3.6 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 5.2/16.2 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 5.2/16.2 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 5.2/16.2 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 5.2/16.2 MB 4.3 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 5.2/16.2 MB 4.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 6.3/16.2 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 7.3/16.2 MB 2.9 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 7.3/16.2 MB 2.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 8.4/16.2 MB 2.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 9.4/16.2 MB 3.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 9.4/16.2 MB 3.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 10.5/16.2 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.3/16.2 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.1/16.2 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 12.6/16.2 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 13.6/16.2 MB 3.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.6/16.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.7/16.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.2/16.2 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.7/16.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.0/16.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.2/16.2 MB 2.8 MB/s  0:00:05\n",
      "Downloading tree_sitter_c_sharp-0.23.1-cp39-abi3-win_amd64.whl (377 kB)\n",
      "Downloading tree_sitter_embedded_template-0.25.0-cp310-abi3-win_amd64.whl (13 kB)\n",
      "Downloading tree_sitter_yaml-0.7.2-cp310-abi3-win_amd64.whl (45 kB)\n",
      "Downloading turbopuffer-1.7.0-py3-none-any.whl (110 kB)\n",
      "Downloading flupy-1.2.3-py3-none-any.whl (12 kB)\n",
      "Downloading pgvector-0.3.6-py3-none-any.whl (24 kB)\n",
      "Downloading psycopg2_binary-2.9.11-cp312-cp312-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.3/2.7 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.6/2.7 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 5.6 MB/s  0:00:00\n",
      "Downloading voyageai-0.3.5-py3-none-any.whl (28 kB)\n",
      "Downloading weaviate_client-4.18.1-py3-none-any.whl (598 kB)\n",
      "   ---------------------------------------- 0.0/598.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 598.1/598.1 kB 3.6 MB/s  0:00:00\n",
      "Downloading authlib-1.6.5-py2.py3-none-any.whl (243 kB)\n",
      "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
      "Downloading aiolimiter-1.2.1-py3-none-any.whl (6.7 kB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Downloading pinecone-8.0.0-py3-none-any.whl (745 kB)\n",
      "   ---------------------------------------- 0.0/745.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 745.9/745.9 kB 4.4 MB/s  0:00:00\n",
      "Downloading pinecone_plugin_assistant-3.0.1-py3-none-any.whl (280 kB)\n",
      "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Downloading pymongo-4.15.4-cp312-cp312-win_amd64.whl (910 kB)\n",
      "   ---------------------------------------- 0.0/910.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 910.9/910.9 kB 4.6 MB/s  0:00:00\n",
      "Downloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
      "Building wheels for collected packages: vecs\n",
      "  Building wheel for vecs (pyproject.toml): started\n",
      "  Building wheel for vecs (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for vecs: filename=vecs-0.4.5-py3-none-any.whl size=23957 sha256=db8d0489ea3610cabd63eb38fabeb7a692d26135f8c4212c4716509894ae4c5c\n",
      "  Stored in directory: c:\\users\\mahadevaa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local\\pip\\cache\\wheels\\7d\\6f\\b0\\7b179069125cbd65c5f83a084b301f65d4db308de2a751f6d9\n",
      "Successfully built vecs\n",
      "Installing collected packages: pywin32, validators, types-requests, tree-sitter-yaml, tree-sitter-embedded-template, tree-sitter-c-sharp, tree-sitter, PyJWT, psycopg2-binary, portalocker, pinecone-plugin-interface, pgvector, packaging, hyperframe, httpx-sse, hpack, flupy, fastavro, et-xmlfile, elastic-transport, dnspython, aiolimiter, vecs, tree-sitter-language-pack, pymongo, pinecone-plugin-assistant, openpyxl, h2, elasticsearch, deprecation, chonkie, azure-core, turbopuffer, pymilvus, pinecone, onnxruntime, google-genai, authlib, weaviate-client, qdrant-client, msal, model2vec, magika, cohere, msal-extensions, voyageai, azure-identity\n",
      "\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "   ----------------------------------------  0/47 [pywin32]\n",
      "    ---------------------------------------  1/47 [validators]\n",
      "    ---------------------------------------  1/47 [validators]\n",
      "    ---------------------------------------  1/47 [validators]\n",
      "   -- -------------------------------------  3/47 [tree-sitter-yaml]\n",
      "   --- ----------------------------------  4/47 [tree-sitter-embedded-template]\n",
      "   ---- -----------------------------------  5/47 [tree-sitter-c-sharp]\n",
      "   ----- ----------------------------------  7/47 [PyJWT]\n",
      "   ----- ----------------------------------  7/47 [PyJWT]\n",
      "   ------ ---------------------------------  8/47 [psycopg2-binary]\n",
      "   ------ ---------------------------------  8/47 [psycopg2-binary]\n",
      "   ------- --------------------------------  9/47 [portalocker]\n",
      "   -------- ------------------------------- 10/47 [pinecone-plugin-interface]\n",
      "   --------- ------------------------------ 11/47 [pgvector]\n",
      "   --------- ------------------------------ 11/47 [pgvector]\n",
      "   --------- ------------------------------ 11/47 [pgvector]\n",
      "   --------- ------------------------------ 11/47 [pgvector]\n",
      "  Attempting uninstall: packaging\n",
      "   --------- ------------------------------ 11/47 [pgvector]\n",
      "    Found existing installation: packaging 25.0\n",
      "   --------- ------------------------------ 11/47 [pgvector]\n",
      "    Uninstalling packaging-25.0:\n",
      "   --------- ------------------------------ 11/47 [pgvector]\n",
      "      Successfully uninstalled packaging-25.0\n",
      "   --------- ------------------------------ 11/47 [pgvector]\n",
      "   ---------- ----------------------------- 12/47 [packaging]\n",
      "   ---------- ----------------------------- 12/47 [packaging]\n",
      "   ---------- ----------------------------- 12/47 [packaging]\n",
      "   ---------- ----------------------------- 12/47 [packaging]\n",
      "   ----------- ---------------------------- 13/47 [hyperframe]\n",
      "  Attempting uninstall: httpx-sse\n",
      "   ----------- ---------------------------- 13/47 [hyperframe]\n",
      "    Found existing installation: httpx-sse 0.4.3\n",
      "   ----------- ---------------------------- 13/47 [hyperframe]\n",
      "   ----------- ---------------------------- 14/47 [httpx-sse]\n",
      "    Uninstalling httpx-sse-0.4.3:\n",
      "   ----------- ---------------------------- 14/47 [httpx-sse]\n",
      "      Successfully uninstalled httpx-sse-0.4.3\n",
      "   ----------- ---------------------------- 14/47 [httpx-sse]\n",
      "   ----------- ---------------------------- 14/47 [httpx-sse]\n",
      "   ------------ --------------------------- 15/47 [hpack]\n",
      "   ------------ --------------------------- 15/47 [hpack]\n",
      "   -------------- ------------------------- 17/47 [fastavro]\n",
      "   -------------- ------------------------- 17/47 [fastavro]\n",
      "   -------------- ------------------------- 17/47 [fastavro]\n",
      "   --------------- ------------------------ 18/47 [et-xmlfile]\n",
      "   ---------------- ----------------------- 19/47 [elastic-transport]\n",
      "   ---------------- ----------------------- 19/47 [elastic-transport]\n",
      "   ----------------- ---------------------- 20/47 [dnspython]\n",
      "   ----------------- ---------------------- 20/47 [dnspython]\n",
      "   ----------------- ---------------------- 20/47 [dnspython]\n",
      "   ----------------- ---------------------- 20/47 [dnspython]\n",
      "   ----------------- ---------------------- 20/47 [dnspython]\n",
      "   ----------------- ---------------------- 20/47 [dnspython]\n",
      "   ----------------- ---------------------- 20/47 [dnspython]\n",
      "   ----------------- ---------------------- 20/47 [dnspython]\n",
      "   ----------------- ---------------------- 20/47 [dnspython]\n",
      "   ----------------- ---------------------- 20/47 [dnspython]\n",
      "   ----------------- ---------------------- 20/47 [dnspython]\n",
      "   ----------------- ---------------------- 20/47 [dnspython]\n",
      "   ----------------- ---------------------- 20/47 [dnspython]\n",
      "   ------------------ --------------------- 22/47 [vecs]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   ------------------- -------------------- 23/47 [tree-sitter-language-pack]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   -------------------- ------------------- 24/47 [pymongo]\n",
      "   --------------------- ------------------ 25/47 [pinecone-plugin-assistant]\n",
      "   --------------------- ------------------ 25/47 [pinecone-plugin-assistant]\n",
      "   --------------------- ------------------ 25/47 [pinecone-plugin-assistant]\n",
      "   --------------------- ------------------ 25/47 [pinecone-plugin-assistant]\n",
      "   --------------------- ------------------ 25/47 [pinecone-plugin-assistant]\n",
      "   --------------------- ------------------ 25/47 [pinecone-plugin-assistant]\n",
      "   --------------------- ------------------ 25/47 [pinecone-plugin-assistant]\n",
      "   --------------------- ------------------ 25/47 [pinecone-plugin-assistant]\n",
      "   --------------------- ------------------ 25/47 [pinecone-plugin-assistant]\n",
      "   --------------------- ------------------ 25/47 [pinecone-plugin-assistant]\n",
      "   --------------------- ------------------ 25/47 [pinecone-plugin-assistant]\n",
      "   --------------------- ------------------ 25/47 [pinecone-plugin-assistant]\n",
      "   --------------------- ------------------ 25/47 [pinecone-plugin-assistant]\n",
      "   --------------------- ------------------ 25/47 [pinecone-plugin-assistant]\n",
      "   --------------------- ------------------ 25/47 [pinecone-plugin-assistant]\n",
      "   --------------------- ------------------ 25/47 [pinecone-plugin-assistant]\n",
      "   ---------------------- ----------------- 26/47 [openpyxl]\n",
      "   ---------------------- ----------------- 26/47 [openpyxl]\n",
      "   ---------------------- ----------------- 26/47 [openpyxl]\n",
      "   ---------------------- ----------------- 26/47 [openpyxl]\n",
      "   ---------------------- ----------------- 26/47 [openpyxl]\n",
      "   ---------------------- ----------------- 26/47 [openpyxl]\n",
      "   ---------------------- ----------------- 26/47 [openpyxl]\n",
      "   ---------------------- ----------------- 26/47 [openpyxl]\n",
      "   ---------------------- ----------------- 26/47 [openpyxl]\n",
      "   ---------------------- ----------------- 26/47 [openpyxl]\n",
      "   ---------------------- ----------------- 26/47 [openpyxl]\n",
      "   ---------------------- ----------------- 26/47 [openpyxl]\n",
      "   ---------------------- ----------------- 26/47 [openpyxl]\n",
      "   ---------------------- ----------------- 26/47 [openpyxl]\n",
      "   ---------------------- ----------------- 26/47 [openpyxl]\n",
      "   ---------------------- ----------------- 26/47 [openpyxl]\n",
      "   ---------------------- ----------------- 26/47 [openpyxl]\n",
      "   ---------------------- ----------------- 26/47 [openpyxl]\n",
      "   ---------------------- ----------------- 27/47 [h2]\n",
      "   ---------------------- ----------------- 27/47 [h2]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ----------------------- ---------------- 28/47 [elasticsearch]\n",
      "   ------------------------- -------------- 30/47 [chonkie]\n",
      "   ------------------------- -------------- 30/47 [chonkie]\n",
      "   ------------------------- -------------- 30/47 [chonkie]\n",
      "   ------------------------- -------------- 30/47 [chonkie]\n",
      "   ------------------------- -------------- 30/47 [chonkie]\n",
      "   ------------------------- -------------- 30/47 [chonkie]\n",
      "   ------------------------- -------------- 30/47 [chonkie]\n",
      "   ------------------------- -------------- 30/47 [chonkie]\n",
      "   ------------------------- -------------- 30/47 [chonkie]\n",
      "   ------------------------- -------------- 30/47 [chonkie]\n",
      "   ------------------------- -------------- 30/47 [chonkie]\n",
      "   ------------------------- -------------- 30/47 [chonkie]\n",
      "   ------------------------- -------------- 30/47 [chonkie]\n",
      "   ------------------------- -------------- 30/47 [chonkie]\n",
      "   ------------------------- -------------- 30/47 [chonkie]\n",
      "   ------------------------- -------------- 30/47 [chonkie]\n",
      "   -------------------------- ------------- 31/47 [azure-core]\n",
      "   -------------------------- ------------- 31/47 [azure-core]\n",
      "   -------------------------- ------------- 31/47 [azure-core]\n",
      "   -------------------------- ------------- 31/47 [azure-core]\n",
      "   -------------------------- ------------- 31/47 [azure-core]\n",
      "   -------------------------- ------------- 31/47 [azure-core]\n",
      "   -------------------------- ------------- 31/47 [azure-core]\n",
      "   -------------------------- ------------- 31/47 [azure-core]\n",
      "   -------------------------- ------------- 31/47 [azure-core]\n",
      "   --------------------------- ------------ 32/47 [turbopuffer]\n",
      "   --------------------------- ------------ 32/47 [turbopuffer]\n",
      "   --------------------------- ------------ 32/47 [turbopuffer]\n",
      "   --------------------------- ------------ 32/47 [turbopuffer]\n",
      "   --------------------------- ------------ 32/47 [turbopuffer]\n",
      "   --------------------------- ------------ 32/47 [turbopuffer]\n",
      "   --------------------------- ------------ 32/47 [turbopuffer]\n",
      "   ---------------------------- ----------- 33/47 [pymilvus]\n",
      "   ---------------------------- ----------- 33/47 [pymilvus]\n",
      "   ---------------------------- ----------- 33/47 [pymilvus]\n",
      "   ---------------------------- ----------- 33/47 [pymilvus]\n",
      "   ---------------------------- ----------- 33/47 [pymilvus]\n",
      "   ---------------------------- ----------- 33/47 [pymilvus]\n",
      "   ---------------------------- ----------- 33/47 [pymilvus]\n",
      "   ---------------------------- ----------- 33/47 [pymilvus]\n",
      "   ---------------------------- ----------- 33/47 [pymilvus]\n",
      "   ---------------------------- ----------- 33/47 [pymilvus]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "  Attempting uninstall: onnxruntime\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "    Found existing installation: onnxruntime 1.23.2\n",
      "   ---------------------------- ----------- 34/47 [pinecone]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "    Uninstalling onnxruntime-1.23.2:\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "      Successfully uninstalled onnxruntime-1.23.2\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ----------------------------- ---------- 35/47 [onnxruntime]\n",
      "   ------------------------------ --------- 36/47 [google-genai]\n",
      "   ------------------------------ --------- 36/47 [google-genai]\n",
      "   ------------------------------ --------- 36/47 [google-genai]\n",
      "   ------------------------------ --------- 36/47 [google-genai]\n",
      "   ------------------------------ --------- 36/47 [google-genai]\n",
      "   ------------------------------ --------- 36/47 [google-genai]\n",
      "   ------------------------------ --------- 36/47 [google-genai]\n",
      "   ------------------------------ --------- 36/47 [google-genai]\n",
      "   ------------------------------ --------- 36/47 [google-genai]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   ------------------------------- -------- 37/47 [authlib]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   -------------------------------- ------- 38/47 [weaviate-client]\n",
      "   --------------------------------- ------ 39/47 [qdrant-client]\n",
      "   --------------------------------- ------ 39/47 [qdrant-client]\n",
      "   --------------------------------- ------ 39/47 [qdrant-client]\n",
      "   --------------------------------- ------ 39/47 [qdrant-client]\n",
      "   --------------------------------- ------ 39/47 [qdrant-client]\n",
      "   --------------------------------- ------ 39/47 [qdrant-client]\n",
      "   --------------------------------- ------ 39/47 [qdrant-client]\n",
      "   --------------------------------- ------ 39/47 [qdrant-client]\n",
      "   --------------------------------- ------ 39/47 [qdrant-client]\n",
      "   --------------------------------- ------ 39/47 [qdrant-client]\n",
      "   --------------------------------- ------ 39/47 [qdrant-client]\n",
      "   --------------------------------- ------ 39/47 [qdrant-client]\n",
      "   --------------------------------- ------ 39/47 [qdrant-client]\n",
      "   ---------------------------------- ----- 40/47 [msal]\n",
      "   ---------------------------------- ----- 40/47 [msal]\n",
      "   ---------------------------------- ----- 41/47 [model2vec]\n",
      "   ---------------------------------- ----- 41/47 [model2vec]\n",
      "   ----------------------------------- ---- 42/47 [magika]\n",
      "   ----------------------------------- ---- 42/47 [magika]\n",
      "   ----------------------------------- ---- 42/47 [magika]\n",
      "   ----------------------------------- ---- 42/47 [magika]\n",
      "   ----------------------------------- ---- 42/47 [magika]\n",
      "   ----------------------------------- ---- 42/47 [magika]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------ --- 43/47 [cohere]\n",
      "   ------------------------------------- -- 44/47 [msal-extensions]\n",
      "   -------------------------------------- - 45/47 [voyageai]\n",
      "   -------------------------------------- - 45/47 [voyageai]\n",
      "   ---------------------------------------  46/47 [azure-identity]\n",
      "   ---------------------------------------  46/47 [azure-identity]\n",
      "   ---------------------------------------  46/47 [azure-identity]\n",
      "   ---------------------------------------  46/47 [azure-identity]\n",
      "   ---------------------------------------  46/47 [azure-identity]\n",
      "   ---------------------------------------  46/47 [azure-identity]\n",
      "   ---------------------------------------  46/47 [azure-identity]\n",
      "   ---------------------------------------  46/47 [azure-identity]\n",
      "   ---------------------------------------  46/47 [azure-identity]\n",
      "   ---------------------------------------  46/47 [azure-identity]\n",
      "   ---------------------------------------- 47/47 [azure-identity]\n",
      "\n",
      "Successfully installed PyJWT-2.10.1 aiolimiter-1.2.1 authlib-1.6.5 azure-core-1.36.0 azure-identity-1.25.1 chonkie-1.4.2 cohere-5.20.0 deprecation-2.1.0 dnspython-2.8.0 elastic-transport-9.2.0 elasticsearch-9.2.0 et-xmlfile-2.0.0 fastavro-1.12.1 flupy-1.2.3 google-genai-1.52.0 h2-4.3.0 hpack-4.1.0 httpx-sse-0.4.0 hyperframe-6.1.0 magika-1.0.1 model2vec-0.7.0 msal-1.34.0 msal-extensions-1.3.1 onnxruntime-1.20.1 openpyxl-3.1.5 packaging-24.2 pgvector-0.3.6 pinecone-8.0.0 pinecone-plugin-assistant-3.0.1 pinecone-plugin-interface-0.0.7 portalocker-3.2.0 psycopg2-binary-2.9.11 pymilvus-2.6.3 pymongo-4.15.4 pywin32-311 qdrant-client-1.16.0 tree-sitter-0.25.2 tree-sitter-c-sharp-0.23.1 tree-sitter-embedded-template-0.25.0 tree-sitter-language-pack-0.11.0 tree-sitter-yaml-0.7.2 turbopuffer-1.7.0 types-requests-2.32.4.20250913 validators-0.35.0 vecs-0.4.5 voyageai-0.3.5 weaviate-client-4.18.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# For all features\n",
    "%pip install \"chonkie[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "782db1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3deebf91fdee4db2a4b4bffeb8fc0fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/129M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MahadevaA\\.cache\\huggingface\\hub\\models--minishlab--potion-base-32M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f44e4de282b478aa8117532ed5aab49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022c799d9aa94e3ea91a0047b141ba38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/202 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1674c8294f5c42bdb775cdfdf3772043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: Woah! Chonkie, the chunking library is so cool!\n",
      "Tokens: 14\n"
     ]
    }
   ],
   "source": [
    "# First import the chunker you want from Chonkie\n",
    "from chonkie import TokenChunker, SemanticChunker\n",
    "\n",
    "# Initialize the chunker\n",
    "# chunker = TokenChunker() # defaults to using GPT2 tokenizer\n",
    "\n",
    "chunker = SemanticChunker(model_name=\"all-MiniLM-L6-v2\", chunk_size=50, overlap_size=10)\n",
    "\n",
    "# Here's some text to chunk\n",
    "text = \"\"\"Woah! Chonkie, the chunking library is so cool!\"\"\"\n",
    "\n",
    "# Chunk some text\n",
    "chunks = chunker(text)\n",
    "\n",
    "# Access chunks\n",
    "for chunk in chunks:\n",
    "  print(f\"Chunk: {chunk.text}\")\n",
    "  print(f\"Tokens: {chunk.token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eefb3f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 20\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import os\n",
    "\n",
    "file_path = r\"files/ES Mod1@AzDOCUMENTS2.pdf\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"The file at {file_path} does not exist.\")\n",
    "\n",
    "loader = PyMuPDFLoader(file_path)\n",
    "\n",
    "\n",
    "pages = loader.load()\n",
    "\n",
    "print(f\"Number of pages: {len(pages)}\") # 20 Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8114aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import TokenChunker, SemanticChunker, SentenceChunker, TableChunker, CodeChunker, RecursiveChunker, SlumberChunker\n",
    "\n",
    "# chunker = SemanticChunker(model_name=\"all-MiniLM-L6-v2\", chunk_size=50, overlap_size=10)\n",
    "\n",
    "# chunker = SentenceChunker(min_sentences_per_chunk=10)\n",
    "\n",
    "# chunker = TableChunker()\n",
    "\n",
    "# chunks = chunker(pages[0].page_content)\n",
    "\n",
    "\n",
    "\n",
    "for page in pages:\n",
    "  chunks += chunker(page.page_content)\n",
    "\n",
    "for chunk in chunks:\n",
    "  print(f\"Chunk: {chunk.text}\")\n",
    "  print(f\"Tokens: {chunk.token_count}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09f2b0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-22.0.0-cp312-cp312-win_amd64.whl (28.0 MB)\n",
      "   ---------------------------------------- 0.0/28.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/28.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/28.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/28.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/28.0 MB 2.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.6/28.0 MB 3.2 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 2.6/28.0 MB 3.8 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.7/28.0 MB 4.1 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 5.0/28.0 MB 4.5 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 6.8/28.0 MB 5.1 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 9.2/28.0 MB 5.9 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 11.5/28.0 MB 6.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 13.9/28.0 MB 7.0 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 17.0/28.0 MB 7.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 21.2/28.0 MB 8.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 25.2/28.0 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  27.8/28.0 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  27.8/28.0 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.0/28.0 MB 8.6 MB/s  0:00:03\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-22.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Core\n",
    "# %pip install -U chonkie langchain langchain-community langchain-ollama tiktoken\n",
    "# %pip install -U unstructured \"unstructured[pdf]\" python-magic\n",
    "# %pip install faiss-cpu\n",
    "%pip install pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be968c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MahadevaA\\AppData\\Local\\Temp\\ipykernel_27644\\2984793673.py:162: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  created_at=datetime.utcnow()\n",
      "C:\\Users\\MahadevaA\\AppData\\Local\\Temp\\ipykernel_27644\\2984793673.py:163: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  ).dict()\n"
     ]
    },
    {
     "ename": "ArrowKeyError",
     "evalue": "No type extension with name arrow.py_extension_type found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowKeyError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 322\u001b[39m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    316\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mchunks\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(all_chunks),\n\u001b[32m    317\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mavg_tokens\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(np.mean([c[\u001b[33m\"\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m all_chunks])) \u001b[38;5;28;01mif\u001b[39;00m all_chunks \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m,\n\u001b[32m    318\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstores_built\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(vectorstores.keys()) + ([\u001b[33m\"\u001b[39m\u001b[33mchroma_cosine\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m BUILD_CHROMA_COS \u001b[38;5;28;01melse\u001b[39;00m [])\n\u001b[32m    319\u001b[39m     }\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     summary = \u001b[43magentic_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfiles/ES Mod1@AzDOCUMENTS2.pdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 312\u001b[39m, in \u001b[36magentic_chunk\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;66;03m# 5) Metadata table\u001b[39;00m\n\u001b[32m    311\u001b[39m df = pd.DataFrame([c[\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m all_chunks])\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43martifacts/chunk_metadata.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[38;5;66;03m# Return a small summary\u001b[39;00m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    316\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mchunks\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(all_chunks),\n\u001b[32m    317\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mavg_tokens\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(np.mean([c[\u001b[33m\"\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m all_chunks])) \u001b[38;5;28;01mif\u001b[39;00m all_chunks \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m,\n\u001b[32m    318\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mstores_built\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(vectorstores.keys()) + ([\u001b[33m\"\u001b[39m\u001b[33mchroma_cosine\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m BUILD_CHROMA_COS \u001b[38;5;28;01melse\u001b[39;00m [])\n\u001b[32m    319\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:3113\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3032\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3033\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3034\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3109\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3110\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3111\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3114\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3122\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:476\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    475\u001b[39m     partition_cols = [partition_cols]\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m    480\u001b[39m impl.write(\n\u001b[32m    481\u001b[39m     df,\n\u001b[32m    482\u001b[39m     path_or_buf,\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m     **kwargs,\n\u001b[32m    489\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:63\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m engine_class \u001b[38;5;129;01min\u001b[39;00m engine_classes:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     65\u001b[39m         error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\pandas\\io\\parquet.py:169\u001b[39m, in \u001b[36mPyArrowImpl.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;28mself\u001b[39m.api = pyarrow\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py:174\u001b[39m\n\u001b[32m    167\u001b[39m     pyarrow.register_extension_type(\n\u001b[32m    168\u001b[39m         ForbiddenExtensionType(pyarrow.null(), \u001b[33m\"\u001b[39m\u001b[33marrow.py_extension_type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m     )\n\u001b[32m    171\u001b[39m     pyarrow._hotfix_installed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[43mpatch_pyarrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\extension_types.py:166\u001b[39m, in \u001b[36mpatch_pyarrow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    157\u001b[39m         pickletools.dis(serialized, out)\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    159\u001b[39m             _ERROR_MSG.format(\n\u001b[32m    160\u001b[39m                 storage_type=storage_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m    163\u001b[39m             )\n\u001b[32m    164\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[43mpyarrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43munregister_extension_type\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marrow.py_extension_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m pyarrow.register_extension_type(\n\u001b[32m    168\u001b[39m     ForbiddenExtensionType(pyarrow.null(), \u001b[33m\"\u001b[39m\u001b[33marrow.py_extension_type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m )\n\u001b[32m    171\u001b[39m pyarrow._hotfix_installed = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\pyarrow\\types.pxi:2280\u001b[39m, in \u001b[36mpyarrow.lib.unregister_extension_type\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowKeyError\u001b[39m: No type extension with name arrow.py_extension_type found"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, hashlib, json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional, Literal\n",
    "\n",
    "# --- Metadata model\n",
    "from pydantic import BaseModel\n",
    "class ChunkMetadata(BaseModel):\n",
    "    doc_id: str\n",
    "    chunk_id: str\n",
    "    source_path: Optional[str]\n",
    "    mime_type: Optional[str]\n",
    "    page_number: Optional[int]\n",
    "    bbox: Optional[Tuple[float,float,float,float]]\n",
    "    section_title: Optional[str]\n",
    "    content_type: Literal[\"table\",\"narrative\",\"code\",\"header\",\"figure\",\"misc\"]\n",
    "    tokens: int\n",
    "    start_char: int\n",
    "    end_char: int\n",
    "    start_token: Optional[int]\n",
    "    end_token: Optional[int]\n",
    "    hash_sha256: str\n",
    "    vector_metric: Optional[Literal[\"cosine\",\"l2\",\"ip\"]]\n",
    "    embedding_model: Optional[str]\n",
    "    table_schema: Optional[Dict[str, str]]\n",
    "    table_rows: Optional[int]\n",
    "    neighbors: Optional[List[str]]\n",
    "    created_at: datetime\n",
    "\n",
    "# --- Loaders (Unstructured)\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "# Unstructured extracts elements with categories like Table/Title/NarrativeText, etc.  # [5](https://docs.langchain.com/oss/python/integrations/document_loaders/unstructured_file)\n",
    "\n",
    "# --- Chunkers (Chonkie + LangChain)\n",
    "from chonkie import RecursiveChunker, SemanticChunker as ChonkieSemantic, Pipeline  # [2](https://pypi.org/project/chonkie/)\n",
    "from langchain_experimental.text_splitter import SemanticChunker as LC_SemanticChunker  # [3](https://api.python.langchain.com/en/latest/experimental/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html)\n",
    "\n",
    "# --- Embeddings (Ollama)\n",
    "from langchain_ollama import OllamaEmbeddings  # [9](https://pypi.org/project/langchain-ollama/)\n",
    "\n",
    "# --- Vector stores\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# --- Token counting\n",
    "import tiktoken  # [12](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "EMBED_MODEL = \"nomic-embed-text:v1.5\" # ollama pull nomic-embed-text and        # or another Ollama embedding model  # [8](https://docs.langchain.com/oss/python/integrations/providers/ollama)\n",
    "LLM_MODEL   = \"llama3.1\"                # for local reasoning (optional)     # [8](https://docs.langchain.com/oss/python/integrations/providers/ollama)\n",
    "TOKEN_MODEL = \"cl100k_base\"             # for token counting (OpenAI-style)  # [12](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)\n",
    "\n",
    "CHUNK_SIZE  = 1024\n",
    "CHUNK_OVERLAP = 128\n",
    "\n",
    "# Vector configs\n",
    "BUILD_FAISS_L2     = True\n",
    "BUILD_FAISS_IP     = True\n",
    "BUILD_FAISS_COSINE = True\n",
    "BUILD_CHROMA_COS   = True\n",
    "\n",
    "# -------------------------------\n",
    "# Helpers\n",
    "# -------------------------------\n",
    "enc = tiktoken.get_encoding(TOKEN_MODEL)  # model-aware tokenizer  # [12](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    # precise per-chunk token count\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "def sha256_text(text: str) -> str:\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def normalize_vectors(vectors: np.ndarray) -> np.ndarray:\n",
    "    # unit L2 normalization for cosine via IP in FAISS  # [10](https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances)\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True) + 1e-12\n",
    "    return vectors / norms\n",
    "\n",
    "# -------------------------------\n",
    "# Table-aware chunking\n",
    "# -------------------------------\n",
    "def tabular_chunk(table_text: str, schema: Dict[str,str], doc_id: str, source_path: str, page_number: Optional[int], bbox: Optional[Tuple[float,float,float,float]]) -> List[Dict]:\n",
    "    # Split rows into blocks constrained by token limit; preserve header + schema\n",
    "    rows = table_text.strip().splitlines()\n",
    "    header = rows[0] if rows else \"\"\n",
    "    chunks = []\n",
    "    buf = [header]\n",
    "    start_idx = 0\n",
    "    for i, row in enumerate(rows[1:], start=1):\n",
    "        candidate = \"\\n\".join(buf + [row])\n",
    "        if count_tokens(candidate) > CHUNK_SIZE:\n",
    "            text = \"\\n\".join(buf)\n",
    "            tokens = count_tokens(text)\n",
    "            chunk_id = f\"{doc_id}:table:{len(chunks)}\"\n",
    "            chunks.append({\n",
    "                \"text\": text,\n",
    "                \"tokens\": tokens,\n",
    "                \"start_char\": 0, \"end_char\": len(text),\n",
    "                \"metadata\": ChunkMetadata(\n",
    "                    doc_id=doc_id, chunk_id=chunk_id, source_path=source_path,\n",
    "                    mime_type=None, page_number=page_number, bbox=bbox,\n",
    "                    section_title=None, content_type=\"table\",\n",
    "                    tokens=tokens, start_char=0, end_char=len(text),\n",
    "                    start_token=None, end_token=None,\n",
    "                    hash_sha256=sha256_text(text),\n",
    "                    vector_metric=\"cosine\", embedding_model=EMBED_MODEL,\n",
    "                    table_schema=schema, table_rows=i,\n",
    "                    neighbors=None, created_at=datetime.utcnow()\n",
    "                ).dict()\n",
    "            })\n",
    "            buf = [header, row]\n",
    "        else:\n",
    "            buf.append(row)\n",
    "    if buf:\n",
    "        text = \"\\n\".join(buf)\n",
    "        tokens = count_tokens(text)\n",
    "        chunk_id = f\"{doc_id}:table:{len(chunks)}\"\n",
    "        chunks.append({\n",
    "            \"text\": text,\n",
    "            \"tokens\": tokens,\n",
    "            \"start_char\": 0, \"end_char\": len(text),\n",
    "            \"metadata\": ChunkMetadata(\n",
    "                doc_id=doc_id, chunk_id=chunk_id, source_path=source_path,\n",
    "                mime_type=None, page_number=page_number, bbox=bbox,\n",
    "                section_title=None, content_type=\"table\",\n",
    "                tokens=tokens, start_char=0, end_char=len(text),\n",
    "                start_token=None, end_token=None,\n",
    "                hash_sha256=sha256_text(text),\n",
    "                vector_metric=\"cosine\", embedding_model=EMBED_MODEL,\n",
    "                table_schema=schema, table_rows=len(rows)-1,\n",
    "                neighbors=None, created_at=datetime.utcnow()\n",
    "            ).dict()\n",
    "        })\n",
    "    return chunks\n",
    "\n",
    "# -------------------------------\n",
    "# Narrative/story chunking (Semantic)\n",
    "# -------------------------------\n",
    "def narrative_semantic_chunks(text: str, embeddings) -> List[Dict]:\n",
    "    # LangChain SemanticChunker uses embedding-based breakpoints  # [3](https://api.python.langchain.com/en/latest/experimental/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html)\n",
    "    text_splitter = LC_SemanticChunker(embeddings=embeddings, add_start_index=True)\n",
    "    docs = text_splitter.create_documents([text])\n",
    "    results = []\n",
    "    for i, d in enumerate(docs):\n",
    "        t = d.page_content\n",
    "        tokens = count_tokens(t)\n",
    "        md = ChunkMetadata(\n",
    "            doc_id=\"doc\", chunk_id=f\"doc:narrative:{i}\",\n",
    "            source_path=None, mime_type=None, page_number=None, bbox=None,\n",
    "            section_title=None, content_type=\"narrative\",\n",
    "            tokens=tokens, start_char=d.metadata.get(\"start_index\", 0),\n",
    "            end_char=d.metadata.get(\"start_index\", 0) + len(t),\n",
    "            start_token=None, end_token=None,\n",
    "            hash_sha256=sha256_text(t),\n",
    "            vector_metric=\"cosine\", embedding_model=EMBED_MODEL,\n",
    "            table_schema=None, table_rows=None, neighbors=None,\n",
    "            created_at=datetime.utcnow()\n",
    "        ).dict()\n",
    "        results.append({\"text\": t, \"tokens\": tokens, \"start_char\": md[\"start_char\"], \"end_char\": md[\"end_char\"], \"metadata\": md})\n",
    "    return results\n",
    "\n",
    "# -------------------------------\n",
    "# Orchestrator\n",
    "# -------------------------------\n",
    "def agentic_chunk(file_path: str):\n",
    "    # 1) Load with Unstructured  # extracts element categories incl. tables  # [5](https://docs.langchain.com/oss/python/integrations/document_loaders/unstructured_file)\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # 2) Init embeddings (Ollama)\n",
    "    embeddings = OllamaEmbeddings(model=EMBED_MODEL)  # local embeddings  # [8](https://docs.langchain.com/oss/python/integrations/providers/ollama)\n",
    "\n",
    "    doc_id = os.path.basename(file_path)\n",
    "    all_chunks = []\n",
    "\n",
    "    for d in docs:\n",
    "        content = d.page_content or \"\"\n",
    "        meta = d.metadata or {}\n",
    "        page = meta.get(\"page_number\")\n",
    "        category = meta.get(\"category\", \"NarrativeText\")\n",
    "        bbox = meta.get(\"coordinates\", {}).get(\"points\")\n",
    "\n",
    "        if \"Table\" in category or \"table\" in category.lower():\n",
    "            # Convert element to tabular text; Unstructured may give structured parts  # [5](https://docs.langchain.com/oss/python/integrations/document_loaders/unstructured_file)\n",
    "            schema = {}  # you can infer via header parsing/type hints\n",
    "            table_chunks = tabular_chunk(content, schema, doc_id, file_path, page, bbox)\n",
    "            all_chunks.extend(table_chunks)\n",
    "\n",
    "        elif \"Title\" in category or \"NarrativeText\" in category or \"Paragraph\" in category:\n",
    "            # Narrative/story  semantic chunker\n",
    "            all_chunks.extend(narrative_semantic_chunks(content, embeddings))\n",
    "\n",
    "        elif \"Code\" in category or \"code\" in category.lower():\n",
    "            # Markdown/code  Chonkie Recursive (markdown recipe)  # [2](https://pypi.org/project/chonkie/)\n",
    "            chonk = RecursiveChunker(chunk_size=CHUNK_SIZE, recipe=\"markdown\", overlap=CHUNK_OVERLAP)\n",
    "            chunks = chonk(content)\n",
    "            for i, c in enumerate(chunks):\n",
    "                t = c.text\n",
    "                tokens = c.token_count if hasattr(c, \"token_count\") else count_tokens(t)\n",
    "                md = ChunkMetadata(\n",
    "                    doc_id=doc_id, chunk_id=f\"{doc_id}:code:{i}\",\n",
    "                    source_path=file_path, mime_type=None, page_number=page, bbox=bbox,\n",
    "                    section_title=None, content_type=\"code\",\n",
    "                    tokens=tokens, start_char=c.start_index, end_char=c.end_index,\n",
    "                    start_token=None, end_token=None,\n",
    "                    hash_sha256=sha256_text(t),\n",
    "                    vector_metric=\"cosine\", embedding_model=EMBED_MODEL,\n",
    "                    table_schema=None, table_rows=None, neighbors=None,\n",
    "                    created_at=datetime.utcnow()\n",
    "                ).dict()\n",
    "                all_chunks.append({\"text\": t, \"tokens\": tokens, \"start_char\": c.start_index, \"end_char\": c.end_index, \"metadata\": md})\n",
    "\n",
    "        else:\n",
    "            # Default fallback: Chonkie Semantic chunking  # [2](https://pypi.org/project/chonkie/)\n",
    "            chonk = ChonkieSemantic(chunk_size=CHUNK_SIZE)\n",
    "            chunks = chonk(content)\n",
    "            for i, c in enumerate(chunks):\n",
    "                t = c.text\n",
    "                tokens = c.token_count if hasattr(c, \"token_count\") else count_tokens(t)\n",
    "                md = ChunkMetadata(\n",
    "                    doc_id=doc_id, chunk_id=f\"{doc_id}:misc:{i}\",\n",
    "                    source_path=file_path, mime_type=None, page_number=page, bbox=bbox,\n",
    "                    section_title=None, content_type=\"misc\",\n",
    "                    tokens=tokens, start_char=c.start_index, end_char=c.end_index,\n",
    "                    start_token=None, end_token=None,\n",
    "                    hash_sha256=sha256_text(t),\n",
    "                    vector_metric=\"cosine\", embedding_model=EMBED_MODEL,\n",
    "                    table_schema=None, table_rows=None, neighbors=None,\n",
    "                    created_at=datetime.utcnow()\n",
    "                ).dict()\n",
    "                all_chunks.append({\"text\": t, \"tokens\": tokens, \"start_char\": c.start_index, \"end_char\": c.end_index, \"metadata\": md})\n",
    "\n",
    "    # 3) Persist\n",
    "    os.makedirs(\"artifacts\", exist_ok=True)\n",
    "    # JSON can't serialize datetime objects by default  provide a small helper that serializes datetimes to ISO strings.\n",
    "    def _json_default(obj):\n",
    "        if isinstance(obj, datetime):\n",
    "            return obj.isoformat()\n",
    "        raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "    with open(\"artifacts/chunks.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for c in all_chunks:\n",
    "            f.write(json.dumps(c, default=_json_default, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # 4) Build vector indices (cosine, l2, ip)\n",
    "    texts = [c[\"text\"] for c in all_chunks]\n",
    "    ids   = [c[\"metadata\"][\"chunk_id\"] for c in all_chunks]\n",
    "\n",
    "    # Embed\n",
    "    vecs = embeddings.embed_documents(texts)  # list[List[float]]  # [8](https://docs.langchain.com/oss/python/integrations/providers/ollama)\n",
    "    vecs_np = np.array(vecs, dtype=np.float32)\n",
    "\n",
    "    # FAISS L2\n",
    "    vectorstores = {}\n",
    "    if BUILD_FAISS_L2:\n",
    "        vectorstores[\"faiss_l2\"] = FAISS.from_embeddings(\n",
    "            list(zip(texts, vecs_np)),\n",
    "            embedding=embeddings,\n",
    "            distance_strategy=\"EUCLIDEAN\"  # L2  # FAISS typical usage with L2  # [10](https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances)\n",
    "        )\n",
    "\n",
    "    # FAISS IP (dot product)\n",
    "    if BUILD_FAISS_IP:\n",
    "        vectorstores[\"faiss_ip\"] = FAISS.from_embeddings(\n",
    "            list(zip(texts, vecs_np)),\n",
    "            embedding=embeddings,\n",
    "            distance_strategy=\"MAX_INNER_PRODUCT\"  # dot product  # [10](https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances)\n",
    "        )\n",
    "\n",
    "    # FAISS cosine via normalization + IP\n",
    "    if BUILD_FAISS_COSINE:\n",
    "        vecs_norm = normalize_vectors(vecs_np)  # unit vectors  # [10](https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances)\n",
    "        vectorstores[\"faiss_cosine\"] = FAISS.from_embeddings(\n",
    "            list(zip(texts, vecs_norm)),\n",
    "            embedding=embeddings,\n",
    "            distance_strategy=\"MAX_INNER_PRODUCT\"  # IP  cosine for normalized vectors  # [10](https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances)\n",
    "        )\n",
    "\n",
    "    # Chroma with cosine\n",
    "    if BUILD_CHROMA_COS:\n",
    "        client = chromadb.Client(Settings(persist_directory=\"artifacts/chroma\"))\n",
    "        collection = client.get_or_create_collection(\n",
    "            name=\"chunks\",\n",
    "            metadata={\"hnsw:space\": \"cosine\"}  # cosine metric  # [11](https://docs.trychroma.com/docs/collections/configure)\n",
    "        )\n",
    "        # Chroma requires metadata values to be JSON-serializable primitives.\n",
    "        # Convert any datetime objects (e.g., created_at) to ISO strings before upsert.\n",
    "        def _serialize_metadata(meta: dict) -> dict:\n",
    "            out = {}\n",
    "            for k, v in meta.items():\n",
    "                if isinstance(v, datetime):\n",
    "                    out[k] = v.isoformat()\n",
    "                else:\n",
    "                    out[k] = v\n",
    "            return out\n",
    "\n",
    "        metadatas = [_serialize_metadata(c[\"metadata\"]) for c in all_chunks]\n",
    "        collection.upsert(\n",
    "            ids=ids,\n",
    "            embeddings=vecs,\n",
    "            documents=texts,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "\n",
    "    # 5) Metadata table\n",
    "    df = pd.DataFrame([c[\"metadata\"] for c in all_chunks])\n",
    "    df.to_parquet(\"artifacts/chunk_metadata.parquet\", index=False)\n",
    "\n",
    "    # Return a small summary\n",
    "    return {\n",
    "        \"chunks\": len(all_chunks),\n",
    "        \"avg_tokens\": float(np.mean([c[\"tokens\"] for c in all_chunks])) if all_chunks else 0.0,\n",
    "        \"stores_built\": list(vectorstores.keys()) + ([\"chroma_cosine\"] if BUILD_CHROMA_COS else [])\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    summary = agentic_chunk(\"files/ES Mod1@AzDOCUMENTS2.pdf\")\n",
    "    print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
