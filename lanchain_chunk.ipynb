{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "691dcbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (1.26.6)\n",
      "Requirement already satisfied: langchain in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (1.0.8)\n",
      "Requirement already satisfied: langchain-text-splitters in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (1.0.7)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (2.0.44)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (3.13.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (0.4.44)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (2.3.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.12.4)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (25.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain) (1.0.3)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-community pymupdf langchain langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd6ba4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.pdf import PyMuPDFLoader\n",
    "\n",
    "# load document\n",
    " \n",
    "loader = PyMuPDFLoader(\"ES Mod1@AzDOCUMENTS2.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "print(len(pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75892c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Embedded Systems-18EC62 \n",
      "Azdocuments.in\n",
      "www.azdocuments.in \n",
      "Page 1 \n",
      " \n",
      " \n",
      " \n",
      "EMBEDDED SYSTEMS \n",
      " Course Code : 18EC62 \n",
      " CIE Marks :40 \n",
      " Lecture Hours/Week : 03 + 2 (Tutorial) \n",
      " SEE marks :60 \n",
      " Total Number of Lecture Hours : 50 (10 Hrs / Module)  \n",
      " Exam Hours : 03 \n",
      " CREDITS : 04 \n",
      "MODULE -1 \n",
      "ARM-32 bit Microcontroller: Thumb-2 technology and applications of ARM,Architecture of \n",
      "ARM Cortex M3, Various Units in the architecture, Debuggingsupport, General Purpose \n",
      "Registers, Special Registers, exceptions, interrupts,stack operation, reset sequence (Text 1: \n",
      "Ch-l, 2, 3) \n",
      " \n",
      "ARM -32-BIT MICROCONTROLLER \n",
      " \n",
      " About ARM Cortex-M3 PROCESSOR \n",
      "The microcontroller market is vast, with more than 20 billion devices per year estimated to be shipped in \n",
      "2010. A bewildering array of vendors, devices, and architectures is competing in this market. The \n",
      "requirement for higher performance microcontrollers has been driven globally by the industry’s changing \n",
      "needs; for example, microcontrollers are required to handle more work without increasing a product’s \n",
      "frequency or power. In addition, microcontrollers are becoming increasingly connected, whether by \n",
      "Universal Serial Bus (USB), Ethernet, or wireless radio, and hence, the processing needed to support these \n",
      "communication channels and advanced peripherals are growing. \n",
      "The ARM Cortex™-M3 processor, the first of the Cortex generation of processors released by ARM in \n",
      "2006, was primarily designed to target the 32-bit microcontroller market. The Cortex- M3 processor  \n",
      "provides excellent performance at low gate count and comes with many new features previously available \n",
      "only in high-end processors. The Cortex-M3 addresses the requirements for the 32-bit embedded processor \n",
      "market in the following ways: \n",
      " Greater performance efficiency: allowing more work to be done without increasing the frequency or \n",
      "power requirements \n",
      " Low power consumption: enabling longer battery life, especially critical in portable products \n",
      "including wireless networking applications. \n",
      "  Enhanced determinism: guaranteeing that  critical  tasks  and  interrupts  are  serviced  as quickly' metadata={'producer': '3-Heights™ PDF Toolbox API 6.12.0.6 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2021-07-16T08:05:02+05:30', 'source': 'ES Mod1@AzDOCUMENTS2.pdf', 'file_path': 'ES Mod1@AzDOCUMENTS2.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Microsoft Word - 18EC62-ES-MODULE-1-Notes.docx', 'author': 'Az', 'subject': '', 'keywords': '', 'moddate': '2021-07-16T06:20:37+00:00', 'trapped': '', 'modDate': 'D:20210716062037Z', 'creationDate': \"D:20210716080502+05'30'\", 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c098ae26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 142\n",
      "Avg: 322\n",
      "Max: 588\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "    text,\n",
    "    disallowed_special=()\n",
    ")\n",
    "    return len(tokens)\n",
    "tiktoken.encoding_for_model('gpt-4.1-mini')\n",
    "\n",
    "# create the length function\n",
    "token_counts = []\n",
    "for page in pages:\n",
    "    token_counts.append(tiktoken_len(page.page_content))\n",
    "min_token_count = min(token_counts)\n",
    "avg_token_count = int(sum(token_counts) / len(token_counts))\n",
    "max_token_count = max(token_counts)\n",
    "\n",
    "# print token counts\n",
    "print(f\"Min: {min_token_count}\")\n",
    "print(f\"Avg: {avg_token_count}\")\n",
    "print(f\"Max: {max_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62a379dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 41\n",
      "page_content='Embedded Systems-18EC62 \n",
      "Azdocuments.in\n",
      "www.azdocuments.in \n",
      "Page 1 \n",
      " \n",
      " \n",
      " \n",
      "EMBEDDED SYSTEMS \n",
      " Course Code : 18EC62 \n",
      " CIE Marks :40 \n",
      " Lecture Hours/Week : 03 + 2 (Tutorial) \n",
      " SEE marks :60 \n",
      " Total Number of Lecture Hours : 50 (10 Hrs / Module)  \n",
      " Exam Hours : 03 \n",
      " CREDITS : 04 \n",
      "MODULE -1 \n",
      "ARM-32 bit Microcontroller: Thumb-2 technology and applications of ARM,Architecture of \n",
      "ARM Cortex M3, Various Units in the architecture, Debuggingsupport, General Purpose \n",
      "Registers, Special Registers, exceptions, interrupts,stack operation, reset sequence (Text 1: \n",
      "Ch-l, 2, 3) \n",
      " \n",
      "ARM -32-BIT MICROCONTROLLER \n",
      " \n",
      " About ARM Cortex-M3 PROCESSOR \n",
      "The microcontroller market is vast, with more than 20 billion devices per year estimated to be shipped in \n",
      "2010. A bewildering array of vendors, devices, and architectures is competing in this market. The \n",
      "requirement for higher performance microcontrollers has been driven globally by the industry’s changing' metadata={'producer': '3-Heights™ PDF Toolbox API 6.12.0.6 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2021-07-16T08:05:02+05:30', 'source': 'ES Mod1@AzDOCUMENTS2.pdf', 'file_path': 'ES Mod1@AzDOCUMENTS2.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Microsoft Word - 18EC62-ES-MODULE-1-Notes.docx', 'author': 'Az', 'subject': '', 'keywords': '', 'moddate': '2021-07-16T06:20:37+00:00', 'trapped': '', 'modDate': 'D:20210716062037Z', 'creationDate': \"D:20210716080502+05'30'\", 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# split documents into text and embeddings\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "   chunk_size=1000, \n",
    "   chunk_overlap=200,\n",
    "   length_function=len,\n",
    "   is_separator_regex=False\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af0b21b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad0c0054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 20\n",
      "page_content='Embedded Systems-18EC62 \n",
      "Azdocuments.in\n",
      "www.azdocuments.in \n",
      "Page 1 \n",
      " \n",
      " \n",
      " \n",
      "EMBEDDED SYSTEMS \n",
      " Course Code : 18EC62 \n",
      " CIE Marks :40 \n",
      " Lecture Hours/Week : 03 + 2 (Tutorial) \n",
      " SEE marks :60 \n",
      " Total Number of Lecture Hours : 50 (10 Hrs / Module)  \n",
      " Exam Hours : 03 \n",
      " CREDITS : 04 \n",
      "MODULE -1 \n",
      "ARM-32 bit Microcontroller: Thumb-2 technology and applications of ARM,Architecture of \n",
      "ARM Cortex M3, Various Units in the architecture, Debuggingsupport, General Purpose \n",
      "Registers, Special Registers, exceptions, interrupts,stack operation, reset sequence (Text 1: \n",
      "Ch-l, 2, 3) \n",
      " \n",
      "ARM -32-BIT MICROCONTROLLER \n",
      " \n",
      " About ARM Cortex-M3 PROCESSOR \n",
      "The microcontroller market is vast, with more than 20 billion devices per year estimated to be shipped in \n",
      "2010.\n",
      "\n",
      "A bewildering array of vendors, devices, and architectures is competing in this market.\n",
      "\n",
      "The \n",
      "requirement for higher performance microcontrollers has been driven globally by the industry’s changing \n",
      "needs; for example, microcontrollers are required to handle more work without increasing a product’s \n",
      "frequency or power.\n",
      "\n",
      "In addition, microcontrollers are becoming increasingly connected, whether by \n",
      "Universal Serial Bus (USB), Ethernet, or wireless radio, and hence, the processing needed to support these \n",
      "communication channels and advanced peripherals are growing.\n",
      "\n",
      "The ARM Cortex™-M3 processor, the first of the Cortex generation of processors released by ARM in \n",
      "2006, was primarily designed to target the 32-bit microcontroller market.\n",
      "\n",
      "The Cortex- M3 processor  \n",
      "provides excellent performance at low gate count and comes with many new features previously available \n",
      "only in high-end processors.\n",
      "\n",
      "The Cortex-M3 addresses the requirements for the 32-bit embedded processor \n",
      "market in the following ways: \n",
      " Greater performance efficiency: allowing more work to be done without increasing the frequency or \n",
      "power requirements \n",
      " Low power consumption: enabling longer battery life, especially critical in portable products \n",
      "including wireless networking applications.\n",
      "\n",
      "  Enhanced determinism: guaranteeing that  critical  tasks  and  interrupts  are  serviced  as quickly' metadata={'producer': '3-Heights™ PDF Toolbox API 6.12.0.6 (http://www.pdf-tools.com)', 'creator': '', 'creationdate': '2021-07-16T08:05:02+05:30', 'source': 'ES Mod1@AzDOCUMENTS2.pdf', 'file_path': 'ES Mod1@AzDOCUMENTS2.pdf', 'total_pages': 20, 'format': 'PDF 1.7', 'title': 'Microsoft Word - 18EC62-ES-MODULE-1-Notes.docx', 'author': 'Az', 'subject': '', 'keywords': '', 'moddate': '2021-07-16T06:20:37+00:00', 'trapped': '', 'modDate': 'D:20210716062037Z', 'creationDate': \"D:20210716080502+05'30'\", 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters.nltk import NLTKTextSplitter\n",
    "import nltk\n",
    "\n",
    "# ensure required NLTK data is available\n",
    "nltk.download('punkt', quiet=True)\n",
    "# some NLTK versions/use-cases expect 'punkt_tab' — attempt to download if available\n",
    "try:\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "except Exception:\n",
    "    # ignore if the downloader doesn't recognize this resource name\n",
    "    pass\n",
    "\n",
    "nltk_splitter = NLTKTextSplitter(\n",
    "    language='english'\n",
    ")\n",
    "\n",
    "chunks = nltk_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec4c827",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "OpenAI API key not provided. Set OPENAI_API_KEY environment variable or provide it when prompted.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m         os.environ[\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m] = openai_api_key\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mOpenAI API key not provided. Set OPENAI_API_KEY environment variable or provide it when prompted.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Initialize the embedding model with the API key\u001b[39;00m\n\u001b[32m     18\u001b[39m embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
      "\u001b[31mValueError\u001b[39m: OpenAI API key not provided. Set OPENAI_API_KEY environment variable or provide it when prompted."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Ensure OPENAI_API_KEY is available. You can export it in your environment:\n",
    "#   export OPENAI_API_KEY=\"sk-...\"\n",
    "# Or enter it interactively (won't be echoed) when running this cell.\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    openai_api_key = getpass(\"Enter your OpenAI API key (input hidden): \").strip()\n",
    "    if openai_api_key:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "    else:\n",
    "        raise ValueError(\"OpenAI API key not provided. Set OPENAI_API_KEY environment variable or provide it when prompted.\")\n",
    "\n",
    "# Initialize the embedding model with the API key\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Initialize the SemanticChunker with the embedding model\n",
    "semantic_chunker = SemanticChunker(embeddings=embeddings)\n",
    "\n",
    "# Split the first page into semantic chunks\n",
    "chunks = semantic_chunker.split_text(pages[0].page_content)\n",
    "\n",
    "# Print the resulting chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe972b4a",
   "metadata": {},
   "source": [
    "SemanticChunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "454ad3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67f98e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86a677da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b7ce65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MahadevaA\\AppData\\Local\\Temp\\ipykernel_31708\\4049112824.py:22: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 2. Initialize the SemanticChunker with the embedding model\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# You can tune parameters like breakpoint_threshold_type and amount\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# to control how aggressively the text is split.\u001b[39;00m\n\u001b[32m     27\u001b[39m semantic_chunker = SemanticChunker(\n\u001b[32m     28\u001b[39m     embeddings=embeddings,\n\u001b[32m     29\u001b[39m     breakpoint_threshold_type=\u001b[33m\"\u001b[39m\u001b[33mpercentile\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m# Other options: \"standard_deviation\", \"interquartile\"\u001b[39;00m\n\u001b[32m     30\u001b[39m     breakpoint_threshold_amount=\u001b[32m80\u001b[39m \u001b[38;5;66;03m# Top 20% of semantic changes trigger a split\u001b[39;00m\n\u001b[32m     31\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m semantic_chunks = semantic_chunker.transform_documents(\u001b[43mpages\u001b[49m)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# page1 = pages[0].page_content\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# semantic_chunks = semantic_chunker.transform_documents(pages)\u001b[39;00m\n\u001b[32m     39\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m \n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# # 4. Print the resulting chunks\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(semantic_chunks):\n",
      "\u001b[31mNameError\u001b[39m: name 'pages' is not defined"
     ]
    }
   ],
   "source": [
    "# Install required package for Hugging Face sentence-transformers (fix ImportError)\n",
    "# %pip install -q sentence-transformers\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_community.document_loaders import TextLoader\n",
    "# from langchain_core.documents import Document \n",
    "\n",
    "# Sample text for demonstration\n",
    "# text_data = \"\"\"\n",
    "# Apple's new Vision Pro headset is a spatial computing device that blends digital content with the physical world. \n",
    "# It features a micro-OLED display system and advanced eye-tracking technology.\n",
    "# The company also announced new MacBook Pro models with the M3 chip family, offering significant performance improvements for professional users.\n",
    "# In a separate development, a recent study on climate change highlights the urgent need for global cooperation. \n",
    "# Researchers suggest that renewable energy sources are crucial for mitigating rising temperatures. \n",
    "# The report emphasizes policy changes and sustainable practices.\n",
    "# \"\"\"\n",
    "\n",
    "# 1. Initialize a free embedding model from Hugging Face\n",
    "# This model runs locally and is a good default choice.\n",
    "# model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# 2. Initialize the SemanticChunker with the embedding model\n",
    "# You can tune parameters like breakpoint_threshold_type and amount\n",
    "# to control how aggressively the text is split.\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\", # Other options: \"standard_deviation\", \"interquartile\"\n",
    "    breakpoint_threshold_amount=80 # Top 20% of semantic changes trigger a split\n",
    ")\n",
    "\n",
    "\n",
    "semantic_chunks = semantic_chunker.transform_documents(pages)\n",
    "\n",
    "\n",
    "# page1 = pages[0].page_content\n",
    "# semantic_chunks = semantic_chunker.transform_documents(pages)\n",
    "\n",
    "# print(f\"Number of semantic chunks: {len(semantic_chunks)}\")\n",
    "# print(semantic_chunks[0])\n",
    "\n",
    "\n",
    "\n",
    "# 3. Split the text into documents (chunks)\n",
    "# For a simple string, wrap it in a Document object\n",
    "# documents = [Document(page_content=text_data)]\n",
    "# semantic_chunks = semantic_chunker.split_documents(documents)\n",
    "\n",
    "# # 4. Print the resulting chunks\n",
    "for i, chunk in enumerate(semantic_chunks):\n",
    "    print(f\"--- Chunk {i+1} (Length: {len(chunk.page_content)}) ---\")\n",
    "    print(chunk.page_content)\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96cbc3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f509c37",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not find BertModel neither in <module 'transformers.models.bert' from 'c:\\\\Users\\\\MahadevaA\\\\OneDrive - CXIO Technologies Pvt Ltd\\\\Documents\\\\GitHub\\\\ReSearch\\\\ChunkingStartagies\\\\venv\\\\Lib\\\\site-packages\\\\transformers\\\\models\\\\bert\\\\__init__.py'> nor in <module 'transformers' from 'c:\\\\Users\\\\MahadevaA\\\\OneDrive - CXIO Technologies Pvt Ltd\\\\Documents\\\\GitHub\\\\ReSearch\\\\ChunkingStartagies\\\\venv\\\\Lib\\\\site-packages\\\\transformers\\\\__init__.py'>!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:741\u001b[39m, in \u001b[36mgetattribute_from_module\u001b[39m\u001b[34m(module, attr)\u001b[39m\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:745\u001b[39m, in \u001b[36mgetattribute_from_module\u001b[39m\u001b[34m(module, attr)\u001b[39m\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Could not find BertModel in <module 'transformers' from 'c:\\\\Users\\\\MahadevaA\\\\OneDrive - CXIO Technologies Pvt Ltd\\\\Documents\\\\GitHub\\\\ReSearch\\\\ChunkingStartagies\\\\venv\\\\Lib\\\\site-packages\\\\transformers\\\\__init__.py'>!",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Load tokenizer and model\u001b[39;00m\n\u001b[32m     11\u001b[39m hf_tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m hf_model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_chunk_embedding\u001b[39m(text):\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Tokenize input text\u001b[39;00m\n\u001b[32m     16\u001b[39m     inputs = hf_tokenizer(text, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:601\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    597\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    598\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    599\u001b[39m     )\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping:\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m     model_class = \u001b[43m_get_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:394\u001b[39m, in \u001b[36m_get_model_class\u001b[39m\u001b[34m(config, model_mapping)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     supported_models = \u001b[43mmodel_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    395\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supported_models, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    396\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m supported_models\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:807\u001b[39m, in \u001b[36m_LazyAutoMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_mapping:\n\u001b[32m    806\u001b[39m     model_name = \u001b[38;5;28mself\u001b[39m._model_mapping[model_type]\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[32m    810\u001b[39m model_types = [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._config_mapping.items() \u001b[38;5;28;01mif\u001b[39;00m v == key.\u001b[34m__name__\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:821\u001b[39m, in \u001b[36m_LazyAutoMapping._load_attr_from_module\u001b[39m\u001b[34m(self, model_type, attr)\u001b[39m\n\u001b[32m    819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n\u001b[32m    820\u001b[39m     \u001b[38;5;28mself\u001b[39m._modules[module_name] = importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtransformers.models\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:743\u001b[39m, in \u001b[36mgetattribute_from_module\u001b[39m\u001b[34m(module, attr)\u001b[39m\n\u001b[32m    741\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m getattribute_from_module(transformers_module, attr)\n\u001b[32m    742\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m743\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m neither in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m nor in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    745\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Could not find BertModel neither in <module 'transformers.models.bert' from 'c:\\\\Users\\\\MahadevaA\\\\OneDrive - CXIO Technologies Pvt Ltd\\\\Documents\\\\GitHub\\\\ReSearch\\\\ChunkingStartagies\\\\venv\\\\Lib\\\\site-packages\\\\transformers\\\\models\\\\bert\\\\__init__.py'> nor in <module 'transformers' from 'c:\\\\Users\\\\MahadevaA\\\\OneDrive - CXIO Technologies Pvt Ltd\\\\Documents\\\\GitHub\\\\ReSearch\\\\ChunkingStartagies\\\\venv\\\\Lib\\\\site-packages\\\\transformers\\\\__init__.py'>!"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Example: Using Hugging Face Transformers directly to create chunk vectors (embeddings) for free\n",
    "\n",
    "\n",
    "# Choose a free, widely used sentence transformer model\n",
    "hf_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n",
    "hf_model = AutoModel.from_pretrained(hf_model_name)\n",
    "\n",
    "def get_chunk_embedding(text):\n",
    "    # Tokenize input text\n",
    "    inputs = hf_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        model_output = hf_model(**inputs)\n",
    "    # Mean pooling\n",
    "    embeddings = model_output.last_hidden_state.mean(dim=1)\n",
    "    return embeddings[0].numpy()\n",
    "\n",
    "# Example: create embeddings for all chunks\n",
    "chunk_vectors = [get_chunk_embedding(chunk.page_content) for chunk in semantic_chunks]\n",
    "\n",
    "print(f\"Created {len(chunk_vectors)} chunk vectors.\")\n",
    "print(chunk_vectors[0])  # Show the first vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73025a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d565e6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The sentence_transformers python package is not installed. Please install it with `pip install sentence_transformers`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\chromadb\\utils\\embedding_functions\\sentence_transformer_embedding_function.py:29\u001b[39m, in \u001b[36mSentenceTransformerEmbeddingFunction.__init__\u001b[39m\u001b[34m(self, model_name, device, normalize_embeddings, **kwargs)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\sentence_transformers\\__init__.py:15\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     export_dynamic_quantized_onnx_model,\n\u001b[32m     12\u001b[39m     export_optimized_onnx_model,\n\u001b[32m     13\u001b[39m     export_static_quantized_openvino_model,\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     CrossEncoder,\n\u001b[32m     17\u001b[39m     CrossEncoderModelCardData,\n\u001b[32m     18\u001b[39m     CrossEncoderTrainer,\n\u001b[32m     19\u001b[39m     CrossEncoderTrainingArguments,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mCrossEncoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_card\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:16\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautonotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     AutoConfig,\n\u001b[32m     18\u001b[39m     AutoModelForSequenceClassification,\n\u001b[32m     19\u001b[39m     AutoTokenizer,\n\u001b[32m     20\u001b[39m     PretrainedConfig,\n\u001b[32m     21\u001b[39m     PreTrainedModel,\n\u001b[32m     22\u001b[39m     PreTrainedTokenizer,\n\u001b[32m     23\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'PreTrainedModel' from 'transformers' (c:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\transformers\\__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      9\u001b[39m client = chromadb.Client()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Create or get a collection for your document chunks\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# NOTE: We use the defined model name here.\u001b[39;00m\n\u001b[32m     13\u001b[39m collection = client.get_or_create_collection(\n\u001b[32m     14\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33membedded_systems_chunks\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     embedding_function=\u001b[43membedding_functions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSentenceTransformerEmbeddingFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHF_MODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# --- Prep and Insertion (Assuming semantic_chunks is defined and populated) ---\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Prepare dummy data for insertion\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# documents = [\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m \u001b[38;5;66;03m#     {\"source\": \"wiki\", \"page\": 1}\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[32m     30\u001b[39m documents = [chunk.page_content \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m semantic_chunks]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\chromadb\\utils\\embedding_functions\\sentence_transformer_embedding_function.py:31\u001b[39m, in \u001b[36mSentenceTransformerEmbeddingFunction.__init__\u001b[39m\u001b[34m(self, model_name, device, normalize_embeddings, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     32\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe sentence_transformers python package is not installed. Please install it with `pip install sentence_transformers`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     33\u001b[39m     )\n\u001b[32m     35\u001b[39m \u001b[38;5;28mself\u001b[39m.model_name = model_name\n\u001b[32m     36\u001b[39m \u001b[38;5;28mself\u001b[39m.device = device\n",
      "\u001b[31mValueError\u001b[39m: The sentence_transformers python package is not installed. Please install it with `pip install sentence_transformers`"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# --- Configuration (Assuming these variables are defined upstream) ---\n",
    "# NOTE: Replace 'all-MiniLM-L6-v2' with the model name defined in your hf_model_name variable\n",
    "HF_MODEL_NAME = 'all-MiniLM-L6-v2' \n",
    "\n",
    "# Initialize ChromaDB client (using default settings, stores data locally)\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Create or get a collection for your document chunks\n",
    "# NOTE: We use the defined model name here.\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"embedded_systems_chunks\",\n",
    "    embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(model_name=HF_MODEL_NAME)\n",
    ")\n",
    "\n",
    "# --- Prep and Insertion (Assuming semantic_chunks is defined and populated) ---\n",
    "# Prepare dummy data for insertion\n",
    "# documents = [\n",
    "#     \"The CPU is the brain of an embedded system, executing instructions.\",\n",
    "#     \"Microcontrollers integrate CPU, memory, and peripherals on a single chip.\",\n",
    "#     \"An RTOS manages tasks and resources in real-time constraints.\"\n",
    "# ]\n",
    "# metadatas = [\n",
    "#     {\"source\": \"textbook\", \"page\": 10},\n",
    "#     {\"source\": \"datasheet\", \"page\": 5},\n",
    "#     {\"source\": \"wiki\", \"page\": 1}\n",
    "# ]\n",
    "documents = [chunk.page_content for chunk in semantic_chunks]\n",
    "metadatas = [{\"source\": f\"page_{i+1}\"} for i in range(len(semantic_chunks))]\n",
    "ids = [f\"chunk_{i}\" for i in range(len(documents))]\n",
    "\n",
    "# Add documents and their metadata to the collection\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "print(f\"Stored {len(documents)} chunks in ChromaDB collection 'embedded_systems_chunks'.\")\n",
    "\n",
    "# --- Retrieval with Fix (The fix for your original 'Embeddings: None' issue) ---\n",
    "\n",
    "# Explicitly include \"embeddings\" in the results for inspection\n",
    "results_with_embeddings = collection.get(\n",
    "    include=[\"metadatas\", \"documents\", \"embeddings\"] \n",
    ")\n",
    "\n",
    "ids = results_with_embeddings['ids']\n",
    "metadatas = results_with_embeddings['metadatas']\n",
    "chunk_vectors = results_with_embeddings['embeddings'] \n",
    "\n",
    "# Check if embeddings are now available (they should be, as we requested them)\n",
    "if chunk_vectors is not None and len(chunk_vectors) > 0:\n",
    "    # Print a snippet of the first vector to confirm it's not None\n",
    "    print(f\"\\n--- Retrieval Check ---\")\n",
    "    print(f\"Successfully retrieved embeddings for {len(chunk_vectors)} chunks.\")\n",
    "    print(f\"Embedding of 'chunk_0' starts with: {chunk_vectors[0][:5]}...\") \n",
    "else:\n",
    "    print(\"Error: Embeddings are still None or empty.\")\n",
    "\n",
    "# --- The Core Similarity Search (Querying the RAG system) ---\n",
    "\n",
    "# Define a query string related to the documents\n",
    "query_text = \"What is a single-chip device that controls systems?\"\n",
    "\n",
    "print(f\"\\n--- Similarity Search for: '{query_text}' ---\")\n",
    "\n",
    "# Perform the query\n",
    "# ChromaDB embeds the query_text using the same embedding function \n",
    "# and finds the most similar vectors (k=1)\n",
    "query_results = collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=1, \n",
    "    include=['documents', 'metadatas', 'distances'] # Only include necessary data for the result\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "if query_results and query_results.get('documents') and query_results['documents'][0]:\n",
    "    print(\"\\n✅ Closest Chunk Found:\")\n",
    "    print(f\"Document ID: {query_results['ids'][0][0]}\")\n",
    "    print(f\"Distance (Lower is better): {query_results['distances'][0][0]:.4f}\")\n",
    "    print(f\"Metadata: {query_results['metadatas'][0][0]}\")\n",
    "    print(f\"Content: **{query_results['documents'][0][0]}**\")\n",
    "else:\n",
    "    print(\"No relevant chunks found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e415acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chunk_by_query(query_text, collection, n_results=1):\n",
    "    \"\"\"\n",
    "    Query the ChromaDB collection for the most relevant chunk based on the input query text.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The text query to search for.\n",
    "        collection (chromadb.Collection): The ChromaDB collection containing document chunks.\n",
    "        n_results (int): The number of top results to return.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the retrieved chunk details.\n",
    "    \"\"\"\n",
    "    query_results = collection.query(\n",
    "        query_texts=[query_text],\n",
    "        n_results=n_results,\n",
    "        include=['documents', 'metadatas', 'distances']\n",
    "    )\n",
    "    if query_results and query_results.get('documents') and query_results['documents'][0]:\n",
    "        return {\n",
    "            'id': query_results['ids'][0][0],\n",
    "            'distance': query_results['distances'][0][0],\n",
    "            'metadata': query_results['metadatas'][0][0],\n",
    "            'content': query_results['documents'][0][0]\n",
    "        }\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca7f21e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Query Result:\n",
      "Document ID: chunk_8\n",
      "Distance: 0.3407\n",
      "Metadata: {'source': 'page_9'}\n",
      "Content: ** M  Profile (ARMv7-M):  Processors targeting low-cost applications in which processing  efficiency  \n",
      "is important and cost,  power consumption, low interrupt latency, and ease of use are critical, as well  \n",
      "as industrial control applications, including real-time control systems. The Thumb-2 Technology and Instruction Set Architecture \n",
      " \n",
      "The Thumb-23 technology extended the Thumb Instruction Set Architecture (ISA) into a highly \n",
      "efficient and powerful instruction set that delivers significant benefits in terms of ease of use, code \n",
      "size, and performance (see Figure 1). The extended instruction set in Thumb-2 is a superset of the \n",
      "previous 16-bit Thumb instruction set, with additional 16-bit instructions alongside 32-bit \n",
      "instructions. It allows more complex operations to be carried out in the Thumb state, thus allowing \n",
      "higher efficiency by reducing the number of states switching between ARM state and Thumb state. Fig.1.1 the Relationship between the Thumb Instructions Set in Thumb-2 Technology and the \n",
      "Traditional Thumb \n",
      " \n",
      " \n",
      "With support for both 16-bit and 32-bit instructions in the Thumb-2 instruction set, there is no need to switch \n",
      "the processor between Thumb state (16-bit instructions) and ARM state (32-bit instructions). For example, in \n",
      "ARM7 or ARM9 family processors, you might need to switch to ARM state if you want to  carry out \n",
      "complex calculations or a large number of conditional operations and good performance is needed, whereas \n",
      "in the Cortex-M3 processor, you can mix 32-bit instructions with 16-bit  instructions  without switching \n",
      "state, getting high code density and high performance with no extra complexity.**\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the query function\n",
    "# query_text = \"Briefly discuss how Cortex-M3 address the requirements of the 32-bit embedded processor market\"\n",
    "query_text = \" Briefly explain the Thumb-2 technology and its advantages over thumb instruction set with relevat diagram.\"\n",
    "\n",
    "result = query_chunk_by_query(query_text, collection)\n",
    "if result:\n",
    "    print(\"\\n✅ Query Result:\")\n",
    "    print(f\"Document ID: {result['id']}\")\n",
    "    print(f\"Distance: {result['distance']:.4f}\")\n",
    "    print(f\"Metadata: {result['metadata']}\")\n",
    "    print(f\"Content: **{result['content']}**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b1d126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
