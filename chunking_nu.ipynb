{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b393be8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chonkie in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: langchain-ollama in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: langchain-experimental in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (2.12.4)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (22.0.0)\n",
      "Requirement already satisfied: unstructured in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (0.18.20)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chonkie) (4.67.1)\n",
      "Requirement already satisfied: numpy>=2.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from chonkie) (2.2.6)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (1.1.0)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (2.0.44)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (3.13.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (0.4.44)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from pydantic) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from pydantic) (0.4.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.6.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from langchain-ollama) (0.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: filetype in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: python-magic in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: lxml in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from unstructured) (6.0.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from unstructured) (3.9.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from unstructured) (4.14.2)\n",
      "Requirement already satisfied: emoji in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from unstructured) (2.15.0)\n",
      "Requirement already satisfied: python-iso639 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from unstructured) (2025.11.16)\n",
      "Requirement already satisfied: langdetect in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from unstructured) (3.14.3)\n",
      "Requirement already satisfied: backoff in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: unstructured-client in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from unstructured) (0.42.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from unstructured) (1.17.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from unstructured) (7.1.3)\n",
      "Requirement already satisfied: python-oxmsg in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from unstructured) (0.0.2)\n",
      "Requirement already satisfied: html5lib in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from unstructured) (1.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from tqdm>=4.64.0->chonkie) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from beautifulsoup4->unstructured) (2.8)\n",
      "Requirement already satisfied: webencodings in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from html5lib->unstructured) (0.5.1)\n",
      "Requirement already satisfied: click in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from nltk->unstructured) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from nltk->unstructured) (1.5.2)\n",
      "Requirement already satisfied: olefile in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from python-oxmsg->unstructured) (0.47)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from unstructured-client->unstructured) (25.1.0)\n",
      "Requirement already satisfied: cryptography>=3.1 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from unstructured-client->unstructured) (46.0.3)\n",
      "Requirement already satisfied: pypdf>=6.2.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from unstructured-client->unstructured) (6.3.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from cryptography>=3.1->unstructured-client->unstructured) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mahadevaa\\onedrive - cxio technologies pvt ltd\\documents\\github\\research\\chunkingstartagies\\venv\\lib\\site-packages (from cffi>=2.0.0->cryptography>=3.1->unstructured-client->unstructured) (2.23)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install chonkie langchain-community langchain-ollama langchain-experimental faiss-cpu pandas pydantic tiktoken pyarrow unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "725f8cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "from typing import List, Optional, Dict, Any, Literal\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# --- 1. Metadata Schema (Strict Pydantic Model) ---\n",
    "class ChunkMetadata(BaseModel):\n",
    "    doc_id: str\n",
    "    chunk_id: str\n",
    "    source_path: str\n",
    "    mime_type: str\n",
    "    page_number: Optional[int] = None\n",
    "    bbox: Optional[List[float]] = None # [x1, y1, x2, y2]\n",
    "    section_title: Optional[str] = \"General\"\n",
    "    content_type: Literal[\"narrative\", \"tabular\", \"code\", \"markdown\"]\n",
    "    token_count: int\n",
    "    hash_sha256: str\n",
    "    vector_metric: str = \"cosine\"\n",
    "    embedding_model: str\n",
    "    table_schema: Optional[str] = None # For tabular chunks\n",
    "    neighbors: List[str] = [] # IDs of adjacent chunks\n",
    "    created_at: str = Field(default_factory=lambda: datetime.now().isoformat())\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    text: str\n",
    "    tokens: List[int]\n",
    "    start_char: int\n",
    "    end_char: int\n",
    "    metadata: ChunkMetadata\n",
    "\n",
    "# --- 2. Configuration ---\n",
    "EMBEDDING_MODEL_NAME = \"nomic-embed-text:latest\"\n",
    "HOST_IP = \"10.255.255.254\"\n",
    "OLLAMA_URL = f\"http://{HOST_IP}:11434\"\n",
    "\n",
    "TOKEN_ENCODING = \"cl100k_base\" # Standard for many LLMs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a7690ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MahadevaA\\AppData\\Local\\Temp\\ipykernel_18336\\218148467.py:44: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaEmbeddings``.\n",
      "  ollama_embedder = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME, base_url=OLLAMA_URL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to use Ollama embedder: Error raised by inference endpoint: HTTPConnectionPool(host='10.255.255.254', port=11434): Max retries exceeded with url: /api/embeddings (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002A179BFA900>, 'Connection to 10.255.255.254 timed out. (connect timeout=None)'))\n",
      "Falling back to deterministic DummyEmbeddings.\n",
      "Using DummyEmbeddings. Vector length: 16\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from typing import List\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# --- Define Fallback Embedder (Best Practice) ---\n",
    "# Define the DummyEmbeddings class once, outside the try/except block.\n",
    "# It's also good practice to inherit from LangChain's base Embeddings class.\n",
    "class DummyEmbeddings(Embeddings):\n",
    "    \"\"\"\n",
    "    A deterministic fallback embedder that creates vectors from a SHA256 hash.\n",
    "    This is useful for testing or when a real embedding service is unavailable.\n",
    "    \"\"\"\n",
    "    def __init__(self, length: int = 16):\n",
    "        self.length = length\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        out = []\n",
    "        for t in texts:\n",
    "            h = hashlib.sha256(t.encode(\"utf-8\")).digest()\n",
    "            # Create a deterministic vector from the hash\n",
    "            vec = [((b % 128) - 64) / 64.0 for b in h[:self.length]]\n",
    "            # Normalize the vector\n",
    "            norm = sum(x * x for x in vec) ** 0.5\n",
    "            if norm > 1e-12:\n",
    "                vec = [x / norm for x in vec]\n",
    "            out.append(vec)\n",
    "        return out\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "# --- Initialize Embedder with Fallback Logic ---\n",
    "# These should be defined in your configuration\n",
    "EMBEDDING_MODEL_NAME = \"nomic-embed-text:latest\"\n",
    "HOST_IP = \"10.255.255.254\"\n",
    "OLLAMA_URL = f\"http://{HOST_IP}:11434\"\n",
    "\n",
    "embedder = None\n",
    "\n",
    "try:\n",
    "    # 1. Attempt to initialize and use the Ollama embedder.\n",
    "    # The network call happens on the first use, not during initialization.\n",
    "    ollama_embedder = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME, base_url=OLLAMA_URL)\n",
    "    vec = ollama_embedder.embed_query(\"Hello world\")\n",
    "    embedder = ollama_embedder\n",
    "    print(\"Successfully connected to Ollama and generated embedding.\")\n",
    "    print(\"Vector length:\", len(vec))\n",
    "\n",
    "except Exception as e:\n",
    "    # 2. If any exception occurs (e.g., ConnectionError), fall back.\n",
    "    print(f\"Warning: Failed to use Ollama embedder: {e}\")\n",
    "    print(\"Falling back to deterministic DummyEmbeddings.\")\n",
    "    embedder = DummyEmbeddings()\n",
    "    vec = embedder.embed_query(\"Hello world\")\n",
    "    print(\"Using DummyEmbeddings. Vector length:\", len(vec))\n",
    "\n",
    "# Now, you can use the `embedder` variable throughout your notebook.\n",
    "# It will be either OllamaEmbeddings or DummyEmbeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38eaca3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Embeddings...\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 236\u001b[39m\n\u001b[32m    234\u001b[39m orchestrator = AgenticChunkingOrchestrator()\n\u001b[32m    235\u001b[39m orchestrator.process_document(\u001b[33m\"\u001b[39m\u001b[33mmanual.md\u001b[39m\u001b[33m\"\u001b[39m, sample_text, \u001b[33m\"\u001b[39m\u001b[33mdoc_001\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m \u001b[43morchestrator\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 203\u001b[39m, in \u001b[36mAgenticChunkingOrchestrator.save_results\u001b[39m\u001b[34m(self, output_dir)\u001b[39m\n\u001b[32m    200\u001b[39m metadatas = [ch.metadata.model_dump() \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunks_buffer]\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# Initialize FAISS with Cosine Similarity (Normalize L2)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m vectorstore = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocstore\u001b[49m\u001b[43m=\u001b[49m\u001b[43mInMemoryDocstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_to_docstore_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m vectorstore.save_local(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/faiss_index\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# 4. Report\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1043\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     **kwargs: Any,\n\u001b[32m   1024\u001b[39m ) -> FAISS:\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m     embeddings = \u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__from(\n\u001b[32m   1045\u001b[39m         texts,\n\u001b[32m   1046\u001b[39m         embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1050\u001b[39m         **kwargs,\n\u001b[32m   1051\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\langchain_ollama\\embeddings.py:301\u001b[39m, in \u001b[36mOllamaEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    296\u001b[39m     msg = (\n\u001b[32m    297\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOllama client is not initialized. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    298\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure Ollama is running and the model is loaded.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    299\u001b[39m     )\n\u001b[32m    300\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_default_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkeep_alive\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\ollama\\_client.py:393\u001b[39m, in \u001b[36mClient.embed\u001b[39m\u001b[34m(self, model, input, truncate, options, keep_alive, dimensions)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed\u001b[39m(\n\u001b[32m    385\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    386\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m   dimensions: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    392\u001b[39m ) -> EmbedResponse:\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mEmbedResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/embed\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEmbedRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\ollama\\_client.py:189\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    187\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MahadevaA\\OneDrive - CXIO Technologies Pvt Ltd\\Documents\\GitHub\\ReSearch\\ChunkingStartagies\\venv\\Lib\\site-packages\\ollama\\_client.py:135\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    133\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mConnectionError\u001b[39m: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download"
     ]
    }
   ],
   "source": [
    "from chonkie import TokenChunker, RecursiveChunker, SemanticChunker\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "\n",
    "class AgenticChunkingOrchestrator:\n",
    "    def __init__(self, embedder=None, allow_fallback=True):\n",
    "        \"\"\"\n",
    "        Initialize tokenizer and embeddings.\n",
    "\n",
    "        If Ollama isn't available we'll fall back to a lightweight deterministic\n",
    "        DummyEmbeddings class so the notebook can run locally without requiring\n",
    "        an Ollama server. You can pass a custom embedder to override behavior.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tiktoken.get_encoding(TOKEN_ENCODING)\n",
    "        self.chunks_buffer: List[Chunk] = []\n",
    "        self.report_log: List[str] = []\n",
    "        self.embedder_available = False\n",
    "\n",
    "        # If user provided an embedder, use it\n",
    "        if embedder is not None:\n",
    "            self.embedder = embedder\n",
    "            self.embedder_available = True\n",
    "            return\n",
    "\n",
    "        # Try to instantiate Ollama; if it fails, fall back to a safe dummy embedder\n",
    "        try:\n",
    "            self.embedder = OllamaEmbeddings(\n",
    "                model=EMBEDDING_MODEL_NAME,\n",
    "                base_url=OLLAMA_URL\n",
    "            )\n",
    "            # success\n",
    "            self.embedder_available = True\n",
    "        except Exception as e:\n",
    "            # Keep processing possible â€” add warning to report and optionally use fallback\n",
    "            self.report_log.append(\n",
    "                f\"Warning: Could not initialize OllamaEmbeddings: {e}. Using fallback embeddings.\"\n",
    "            )\n",
    "            if not allow_fallback:\n",
    "                # re-raise so caller can handle the failure explicitly\n",
    "                raise\n",
    "\n",
    "            # Lightweight deterministic fallback embedder (works without external services)\n",
    "            class DummyEmbeddings:\n",
    "                def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "                    out = []\n",
    "                    for t in texts:\n",
    "                        h = hashlib.sha256(t.encode(\"utf-8\")).digest()\n",
    "                        # build a small fixed-size vector (16 dimensions)\n",
    "                        vec = [((b % 128) - 64) / 64.0 for b in h[:16]]\n",
    "                        # L2 normalize\n",
    "                        norm = sum(x * x for x in vec) ** 0.5\n",
    "                        if norm > 1e-12:\n",
    "                            vec = [x / norm for x in vec]\n",
    "                        out.append(vec)\n",
    "                    return out\n",
    "\n",
    "                def embed_query(self, text: str) -> List[float]:\n",
    "                    return self.embed_documents([text])[0]\n",
    "\n",
    "            self.embedder = DummyEmbeddings()\n",
    "            self.embedder_available = False\n",
    "\n",
    "    def _compute_hash(self, text: str) -> str:\n",
    "        return hashlib.sha256(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        return len(self.tokenizer.encode(text))\n",
    "\n",
    "    # --- STRATEGY ROUTER ---\n",
    "    def determine_strategy(self, file_path: str, content_sample: str) -> str:\n",
    "        \"\"\"\n",
    "        Agentic decision making based on file extension and content signals.\n",
    "        \"\"\"\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        \n",
    "        # 1. Code/Markdown Check\n",
    "        if ext in ['.md', '.py', '.js', '.json', '.yaml']:\n",
    "            return \"recursive_markdown\"\n",
    "        \n",
    "        # 2. Tabular Check (CSV or Excel)\n",
    "        if ext in ['.csv', '.xlsx', '.parquet']:\n",
    "            return \"tabular_row\"\n",
    "\n",
    "        # 3. Narrative Check (PDFs, Docx, TXT)\n",
    "        # Simple heuristic: specific keywords or lack of code symbols\n",
    "        if \"table of contents\" in content_sample.lower() or len(content_sample) > 1000:\n",
    "            return \"semantic_narrative\"\n",
    "            \n",
    "        return \"token_fallback\"\n",
    "\n",
    "    # --- CHUNKING ENGINES ---\n",
    "    def process_document(self, file_path: str, text_content: str, doc_id: str):\n",
    "        strategy = self.determine_strategy(file_path, text_content[:2000])\n",
    "        self.report_log.append(f\"Doc: {doc_id} | Strategy: {strategy}\")\n",
    "        \n",
    "        raw_chunks = []\n",
    "        \n",
    "        # Strategy 1: Narrative (Semantic)\n",
    "        if strategy == \"semantic_narrative\":\n",
    "            # Chonkie Semantic Chunker (presuming similarity thresholding)\n",
    "            chunker = SemanticChunker(\n",
    "                embedding_model=self.embedder, \n",
    "                threshold=0.75, \n",
    "                chunk_size=512\n",
    "            )\n",
    "            raw_chunks = chunker.chunk(text_content)\n",
    "\n",
    "        # Strategy 2: Code/Markdown (Recursive)\n",
    "        elif strategy == \"recursive_markdown\":\n",
    "            chunker = RecursiveChunker(\n",
    "                chunk_size=1024,\n",
    "                # chunk_overlap=100 # ~10% overlap\n",
    "            )\n",
    "            raw_chunks = chunker.chunk(text_content)\n",
    "\n",
    "        # Strategy 3: Tabular (Custom Logic)\n",
    "        elif strategy == \"tabular_row\":\n",
    "            # Basic CSV row grouping simulation\n",
    "            rows = text_content.split('\\n')\n",
    "            headers = rows[0]\n",
    "            # Group every 5 rows to maintain context\n",
    "            current_chunk = []\n",
    "            for row in rows[1:]:\n",
    "                current_chunk.append(row)\n",
    "                if len(current_chunk) >= 5:\n",
    "                    raw_chunks.append(f\"Schema: {headers}\\nData:\\n\" + \"\\n\".join(current_chunk))\n",
    "                    current_chunk = []\n",
    "            if current_chunk:\n",
    "                raw_chunks.append(f\"Schema: {headers}\\nData:\\n\" + \"\\n\".join(current_chunk))\n",
    "\n",
    "        # Strategy 4: Fallback\n",
    "        else:\n",
    "            chunker = TokenChunker()\n",
    "            raw_chunks = chunker.chunk(text_content)\n",
    "\n",
    "        # --- ENRICHMENT PHASE ---\n",
    "        self._enrich_and_store(raw_chunks, doc_id, file_path, strategy)\n",
    "\n",
    "    def _enrich_and_store(self, raw_chunks: List[Any], doc_id: str, source: str, strategy: str):\n",
    "        total_chunks = len(raw_chunks)\n",
    "        \n",
    "        for i, chunk_obj in enumerate(raw_chunks):\n",
    "            # Handle Chonkie object vs string\n",
    "            text = chunk_obj.text if hasattr(chunk_obj, 'text') else str(chunk_obj)\n",
    "            start = chunk_obj.start_index if hasattr(chunk_obj, 'start_index') else 0\n",
    "            end = chunk_obj.end_index if hasattr(chunk_obj, 'end_index') else 0\n",
    "            \n",
    "            token_ids = self.tokenizer.encode(text)\n",
    "            count = len(token_ids)\n",
    "            \n",
    "            # Generate ID: doc_id:chunk_index\n",
    "            chunk_id = f\"{doc_id}:ch_{i}\"\n",
    "            \n",
    "            # Identify Neighbors\n",
    "            prev_id = f\"{doc_id}:ch_{i-1}\" if i > 0 else None\n",
    "            next_id = f\"{doc_id}:ch_{i+1}\" if i < total_chunks - 1 else None\n",
    "            neighbors = list(filter(None, [prev_id, next_id]))\n",
    "\n",
    "            meta = ChunkMetadata(\n",
    "                doc_id=doc_id,\n",
    "                chunk_id=chunk_id,\n",
    "                source_path=source,\n",
    "                mime_type=\"text/plain\", # Simplified for demo\n",
    "                content_type=\"tabular\" if \"tabular\" in strategy else \"narrative\",\n",
    "                token_count=count,\n",
    "                hash_sha256=self._compute_hash(text),\n",
    "                embedding_model=EMBEDDING_MODEL_NAME,\n",
    "                vector_metric=\"cosine\",\n",
    "                neighbors=neighbors\n",
    "            )\n",
    "            final_chunk = Chunk(\n",
    "                text=text,\n",
    "                tokens=token_ids,\n",
    "                start_char=start,\n",
    "                end_char=end,\n",
    "                metadata=meta\n",
    "            )\n",
    "            self.chunks_buffer.append(final_chunk)\n",
    "\n",
    "    # --- PERSISTENCE & INDEXING ---\n",
    "    def save_results(self, output_dir=\"output\"):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # 1. JSONL Store\n",
    "        jsonl_path = f\"{output_dir}/chunks.jsonl\"\n",
    "        with open(jsonl_path, 'w') as f:\n",
    "            for ch in self.chunks_buffer:\n",
    "                f.write(ch.model_dump_json() + \"\\n\")\n",
    "        \n",
    "        # 2. Parquet Store (Metadata only for fast scanning)\n",
    "        meta_dicts = [ch.metadata.model_dump() for ch in self.chunks_buffer]\n",
    "        df = pd.DataFrame(meta_dicts)\n",
    "        df.to_parquet(f\"{output_dir}/chunk_metadata.parquet\")\n",
    "\n",
    "        # 3. Vector Indexing (FAISS)\n",
    "        print(\"Generating Embeddings...\")\n",
    "        texts = [ch.text for ch in self.chunks_buffer]\n",
    "        metadatas = [ch.metadata.model_dump() for ch in self.chunks_buffer]\n",
    "        \n",
    "        # Initialize FAISS with Cosine Similarity (Normalize L2)\n",
    "        vectorstore = FAISS.from_texts(\n",
    "            texts=texts,\n",
    "            embedding=self.embedder,\n",
    "            metadatas=metadatas,\n",
    "            docstore=InMemoryDocstore(),\n",
    "            index_to_docstore_id={}\n",
    "        )\n",
    "        vectorstore.save_local(f\"{output_dir}/faiss_index\")\n",
    "\n",
    "        # 4. Report\n",
    "        with open(f\"{output_dir}/chunking_report.md\", \"w\") as f:\n",
    "            f.write(\"# Chunking Strategy Report\\n\\n\")\n",
    "            f.write(f\"**Total Chunks:** {len(self.chunks_buffer)}\\n\")\n",
    "            f.write(\"## Operations Log\\n\")\n",
    "            for log in self.report_log:\n",
    "                f.write(f\"- {log}\\n\")\n",
    "\n",
    "        print(f\"Processing Complete. Outputs saved to {output_dir}/\")\n",
    "\n",
    "# --- EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Mock Data Creation for Demonstration\n",
    "    sample_text = \"\"\"\n",
    "    # Agentic RAG\n",
    "    RAG systems require smart chunking. \n",
    "    \n",
    "    ## Strategy\n",
    "    1. Semantic chunking is best for stories.\n",
    "    2. Recursive is best for code.\n",
    "    \"\"\"\n",
    "    \n",
    "    orchestrator = AgenticChunkingOrchestrator()\n",
    "    orchestrator.process_document(\"manual.md\", sample_text, \"doc_001\")\n",
    "    orchestrator.save_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
