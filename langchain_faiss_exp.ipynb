{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac06c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-openai faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. Setup Environment ---\n",
    "# Make sure to set your OpenAI API key in your environment variables\n",
    "# For example: os.environ[\"OPENAI_API_KEY\"] = \"your_api_key_here\"\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    print(\"Please set your OPENAI_API_KEY environment variable.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Load Documents ---\n",
    "# For this example, let's create a dummy text file.\n",
    "documents_content = \"\"\"\n",
    "The Eiffel Tower, located in Paris, France, is one of the most famous landmarks in the world.\n",
    "It was designed by Gustave Eiffel and completed in 1889. It stands at 330 meters tall.\n",
    "\n",
    "The Great Wall of China is a series of fortifications made of stone, brick, tamped earth, wood, and other materials.\n",
    "It was built to protect the Chinese states and empires against the raids and invasions of the various nomadic groups of the Eurasian Steppe.\n",
    "Several walls were being built as early as the 7th century BC.\n",
    "\"\"\"\n",
    "with open(\"sample_doc.txt\", \"w\") as f:\n",
    "    f.write(documents_content)\n",
    "\n",
    "loader = TextLoader(\"./sample_doc.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# --- 3. Split Documents into Chunks ---\n",
    "# The SemanticChunker from your context is an advanced option.\n",
    "# For simplicity, we'll use a more standard splitter here.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split into {len(split_docs)} chunks.\")\n",
    "for i, doc in enumerate(split_docs):\n",
    "    print(f\"--- Chunk {i+1} ---\\n{doc.page_content}\\n\")\n",
    "\n",
    "# --- 4. Create Embeddings and Store in FAISS ---\n",
    "# We'll use OpenAI embeddings, but you can use any LangChain-compatible embedding model.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# This creates the FAISS vector store from the documents and embeddings.\n",
    "# The documents are embedded and indexed automatically.\n",
    "print(\"Creating FAISS vector store...\")\n",
    "vector_store = FAISS.from_documents(split_docs, embeddings)\n",
    "print(\"FAISS vector store created.\")\n",
    "\n",
    "# --- 5. Perform a Similarity Search ---\n",
    "query = \"How tall is the Eiffel Tower?\"\n",
    "search_results = vector_store.similarity_search(query)\n",
    "\n",
    "print(\"\\n--- Similarity Search Results ---\")\n",
    "for result in search_results:\n",
    "    print(f\"Content: {result.page_content}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "# --- 6. Create a RAG Chain ---\n",
    "# Define the prompt template for the LLM\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Create a chain that will combine the documents into a single string\n",
    "# Create a retriever from our vector store\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Create the final RetrievalQA chain that gets documents, then passes them to the LLM.\n",
    "# Use the \"stuff\" chain type and supply our prompt template via chain_type_kwargs.\n",
    "retrieval_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "print(\"\\n--- Invoking RAG Chain ---\")\n",
    "# RetrievalQA returns a string for a query, so use run()\n",
    "answer = retrieval_chain.run(\"How tall is the Eiffel Tower?\")\n",
    "\n",
    "print(\"\\nAnswer:\")\n",
    "print(answer)\n",
    "print(\"\\nAnswer:\")\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# Clean up the dummy file\n",
    "os.remove(\"sample_doc.txt\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
